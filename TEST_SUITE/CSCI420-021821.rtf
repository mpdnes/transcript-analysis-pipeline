{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1033{\fonttbl{\f0\fnil\fcharset0 Georgia;}{\f1\fnil\fcharset0 Times New Roman;}{\f2\fnil Times New Roman;}{\f3\fnil\fcharset1 Cambria Math;}}
{\*\generator Riched20 10.0.17763}{\*\mmathPr\mmathFont3\mwrapIndent1440 }\viewkind4\uc1 
\pard\tx440\tx880\tx1320\tx1760\tx2200\tx2640\tx3080\tx3520\tx3960\tx4400\tx4840\tx5280\tx5720\tx6160\tx6600\tx7040\tx7480\tx7920\tx8360\tx8800\tx9240\f0\fs22 Principles of Datamining\par
Professor Kinsman\par
CSCI-420-02\par
February 18, 2021 \par
\par
Professor:  Sound check.  \par
\par
Student:   We can hear you.  \par
\par
Professor:  Sounds like it's working.  Very good.  Nice to see folks!  If you haven't said hello, say hello.  \par
\par
I have a general question once you're done signing in and saying hello.  I have a general question that I need your help on.  You as a community -- you the hive mind of 420 students, section 2 -- know something I don't know.  What is a "thin mint?"  \par
\par
Student:   It's a girl scout cookie.  That's chocolate and mint.  \par
\par
Professor:  It's chocolate and mint.  That's what I thought.  \par
\par
Student:   It's the best girl scout cookie.  \par
\par
Professor:  Are they?  You have domain knowledge I don't have.  I don't eat girl scout cookies.  So I wanted to talk you about cookies.  Alexis, thank you very much.  I appreciate it.  I really like class participation and it's very beneficial for me to know that folks are out there.  And I'm not just sitting here talking to myself.  Good.  \par
\par
Excellent.  What do I have to do?  Oh, blast.  I made it too big.  That gets iconic.  Share the screen.  Sharing the screen number 2.  What else do I need to do?  Record.  \par
\par
[Recording in progress.]  \par
\par
Welcome!  So, last time we talked about a bunch of different things.  \par
\par
And I threatened to use your data and give you data about yourselves and then put it out there.  But we discovered there is a problem with that.  I anonymized it by giving people silly names and putting it out there.  What was the problem with that?  \par
\par
What could possibly have gone wrong?  Anybody?  \par
\par
OK so we talked about this concept called de-anonymizing.  Even though it was anonymous we have this problem that it could be de-anonymized.  And this happened famously -- I'll save the stories.  OK?  So there was only one person in the whole class who liked a particular answer.  We could guess from that who they were.  \par
\par
So for example, I give out anonymous data we think is anonymous and we have to be very careful with data what we give out because it could be used backwards and figure out things about somebody else.  I made up a name, Matt, I didn't know we had a Mathew in the class.  Sorry Mathew.  \par
\par
Suppose we have Matthew and he doesn't eat cookies.  Yeah, somebody could lie and say they don't eat cookies.  In the real world, we have to be careful and protect it.  There's datamining laws we'll talk about maybe next Monday or a month from now.  Something like that.  \par
\par
Suppose Matt says he doesn't eat cookies.  Not this one, but some Matt.  It could be Quentin.  Matt is also gluten free and he doesn't want anybody to know he's gluten free.  Looking at the answers, we could figure out who the gluten free person is.  Especially if one person says they don't like cookies.  \par
\par
Once upon a time, AOL put out a bunch of data and said it was anonymous and then people went out and figured out who it was.  They de-anonymized it.  They could figure out each person and their preferences and what they searched for.  Sometimes the things they searched for was embarrassing stuff.  Matt, I hope you're not gluten free.  [Professor Laughing.] \par
\par
It's just an example!  I could have asked your favorite hair color but they were going bald.  We don't know.  If there's one person bald in the classroom, we could have de-anonymized data.  So I'll give you some data that one of my greaters invented about two semesters ago.  It'll be a bigger homework.  It won't be due this Sunday, but next Sunday.  This Sunday I think we should have a quiz.  So there will be another quiz.  \par
\par
I'm gluten free.  I can't digest gluten.  I can't.  I'm missing an enzyme to digest it so I have an issue there.  \par
\par
That's why I don't know what thin mints are.  Last like we talked about using one hot coding.  Remember that?  One hot coding.  So we take the particular -- what happens is we have multi-categorical data and then we want to take that and split it up into different binary variables.  And they must be exclusive of each other.  You really should know that.  What's one hot coding?  What problems does it solves?  \par
\par
It solves that I don't know what the distance between a chocolate chip and a peanut butter and a double chocolate is or a thin mint.  I don't know.  So we give everyone a 1 or a 0 on different categories.  \par
\par
If you have domain knowledge about how people arrange themselves and there's a particular pattern within your population you can make up -- you can invent a number scale.  Using domain knowledge.  For example, some people really like chocolate chip cookies.  And those people who really like chocolate chip cookies are on one end of the spectrum.  On the other end might be oatmeal raisin.  \par
\par
So, what's the problem?  Why do these people not get along well?  These people should never marry each other.  \par
\par
What's the problem?  \par
\par
[Professor Laughing.] \par
\par
Right!  Sam has it!  People look at chocolate chip cookies and think that's a chocolate chip cookie and bite into it.  If you're a huge chocolate chip cookie fan and you're expecting chocolate and you got oatmeal reason, the texture is bad.  Especially those of us that are nerds we have a terrible time with that.  We can be neurosensitive to textures.  I was that way for a long time.  \par
\par
People who really like oatmeal raisin and bite into a chocolate chip -- ugh!  Trust issues.  These are weird tasting.  They like oatmeal raisin and don't like chocolate chip.  \par
\par
The whole point is, using domain knowledge I can make up my own scale and so then I can now compute the distance between a chocolate chip cookie person and an oatmeal raisin person.  The same thing happens with pizza toppings.  Some people really love pineapple on pizza and other people really don't like pineapple on chocolate chip cookies.  OK?  \par
\par
Here's what you'll need to do for the next homework.  The next homework will be much more open.  You're going to be given a bunch of data and it's not real data.  It's synthetic.  You're going to be asked a bunch of questions about it and look through it.  One of the most important things in data science is to get to know your data.  \par
\par
One of the next most important things is to use the correct features.  If we're trying to find clusters for people who want to get along -- one person is a dog person and the other a cat person -- they don't get along.  That's a problem.  We don't want them to marry each other.  We'll invent the next OkCupid.  OK?  \par
\par
What features will we use?  Those might be important but other features might not be important and they should be deleted.  Doesn't matter what your favorite color is, OK?  What distance metric will you use?  You have to choose that.  \par
\par
Then we have to figure out a mathematical model for describing the cluster.  That's a prototype.  What prototype will you use?  How am I going to combine 2 or 3 different data points together into one prototype and describe that?  And we've got to figure out -- pick a linkage when we'll agglomerate.  You'll have to figure that out.  \par
\par
Linkage method.  \par
\par
Oh boy, this will be a pain to grade because you get to pick anything you want as you go along.  I better limit it down.  OK.  So now we have 80 people in 420.  So you've got 80 different data points.  For all of the data points that come in, I'll put every single data point -- data sub index -- into its own cluster.  And its center -- its prototype -- its center of mass will be assigned to that particular data record.  \par
\par
And I have to keep track of what members are inside that cluster.  Initially it's just that first one.  So as we go for all clusters -- idx goes from 1 to 80 -- each cluster has one member in it.  Now, I'm going to repeat until there's only one cluster.  I'll find the two clusters that are closest together.  Find two closest clusters.  Say they are idx and kdx.  Now I'll merge them.  \par
\par
It is the concatenated version of the previous two.  So it gets all the members of whatever I had before and the new ones that are in kdx.  I'm done with kdx.  Delete it.  Cluster idx -- its new center has to be recomputed.  So you have to do that.  Say for example we've got the mode.  We have a bunch of binary variables.  We just want to use the mode.  \par
\par
We might use the mean.  We might use the median.  Something else.  But let's just for the sake pretend we'll use the mode when we merge them together.  \par
\par
Let's look at some data here.  I think I may have made some mistakes.  I made these up like 30 seconds before lecture started.  I think I made mistakes.  Can you help?  \par
\par
Here's some data records.  There are 5 attributes there.  The Jaccard coefficient between 1 and 2 is . . . how many of these bits differ?  Oh, there's one that's different.  How many bits between all of these are there there?  I want you to look at that.  Take 2 minutes and tell me which mistakes.  You can type into the -- see how many mistakes you can find.  I did this quickly.  This gives me a chance to have coffee.  \par
\par
One should be 1 over 2 because there's 2 bits present in 1 and 2.  And then 1 and 3 . . . that's the same.  That's one out of -- oh, maybe that's wrong.  I think 1 and 3 should be \super\f1 1\nosupersub\f2\u8260?\sub\f1 3\nosupersub .\i  \i0\f0  OK.  Yeah.  \par
\par
I'll let you -- what about 1 and 4?  Did I do that one right?  I got them all right?  What's the Jaccard coefficient between 1 to 4?  2 and 4.  Sam says it's \super\f1 2\nosupersub\f2\u8260?\sub\f1 3\nosupersub\i  \i0\f0 and not \super\f1 1\nosupersub\f2\u8260?\sub\f1 3\nosupersub .\i  \i0\f0  There's one different . . . \par
\par
There's two different between 2 to 4 and how many bits?  1 . . . 2 . . . 3.  OK.  So yeah, that's -- OK.  What's for 1 to 4?  Steven says 1 to 4 is one different, two different, three different -- so you said that's?  Steven?  It's \f1\'be\f0 ?  Michael Barton says it's \f1\'be.\i  \i0\f0  I did this quickly.  \par
\par
You should be able to look at this and compute a Jaccard coefficient quickly.  You'll do this for the homework, probably.  Use the Jaccard coefficient.  That's a good way to do it.  Or something else.  Hamming distance or cosine similarity or simple matching coefficient . . . you get to pick.  \par
\par
Got it.  OK so, cool.  Now I've got two I want to merge together.  So the Jaccard coefficient.  Is that a measure of similarity or difference?  If I have a low Jaccard coefficient . . . no, you have to look up what the Jaccard coefficient is.  \par
\par
The Jaccard coefficient is the number of bits that differ divided by the number of bits that are present in either vector.  So 1 and 3 differ by one bit.  That's this bit here on the left.  It's the second bit.  It's different.  And there's 3 bits present.  So its Jaccard coefficient is \super\f1 1\nosupersub\f2\u8260?\sub\f1 3\nosupersub .\i  \i0\f0  Do I want the lowest -- I want the most similar vectors.  So I think I want a low Jaccard coefficient.  \par
\par
It's a measure of difference but it ignores all the 0,0s.  So these are most similar.  We're going to merge them together.  When we do that, C\sub 1\nosupersub  -- the new one -- oh, 1 and 3 get merged together.  So I'll look at the mode for this today.  I'm using the mode.  0,0 is 0.  0 and 1 is .5.  OK.  So this is the new merged version.  Uh oh.  \par
\par
I was hoping I would only have 0s or 1s.  But there's a tie breaking condition for the mode that says if you have an even number, you take the average.  So it's 0.5 there.  OK.  That's going to make life hard to use the Jaccard coefficient.  But oh well.  \par
\par
2 stays the same and the new version of 3 becomes the old version of 4.  So the old cluster 4 becomes cluster 3.  So maybe the mode is not the best thing to do.  Thankfully, I don't have to figure it out!  You get to!  It's an open-ended homework assignment.  \par
\par
So that's a framework to understanding what we're going to do.  \par
\par
[Recording stopped.]  \par
\par
I'm just going to pause for a second.  \par
\par
[Recording in progress.]  \par
\par
So you are a graduate of my course.  You're out walking around in the halls and you bump into one of the other professors.  And they say, "You've been through Dr. K's class on clustering and classification.  What's the difference between them?  Why would I want to cluster my data?"  You should be able to answer that right now in the chat line.  \par
\par
You should all the answer to that right now.  OK?  And the answer is . . . it's to find the structure that's hidden in my data.  I'm looking for the lumps in my data.  I want to find the structures and simplify my data.  I want to know how to talk about my data in meaningful ways.  \par
\par
For example, when I worked at Kodak -- somebody buys a new camera and it was a time when the camera was not built into a cellphone.  Cellphones were just becoming available in the United States in the 2000 time frame.  So you'd buy a handheld camera and have registration.  People would send them out.  You send out a registration card and you get a warranty for one year or whatever it is.  So everybody sent in their card but that would answer questions about themselves.  \par
\par
Looking at that data, we found about 5 types of clusters.  Those were the ones we wanted to.  I don't remember them all, but there was one cluster that bought them from Kodak because it was a quality company.  There was another cluster of people who would buy the cameras because they had the highest number of megapixels possible.  They were the technophiles.  They want the best technology.  \par
\par
And the third group I remember is the people who bought it because it was pink.  They bought it because it was a status symbol to have a digital camera.  It's like driving a Mercedes.  I hope none of you drive those but oh well.  It's a status symbol.  It gets you from place to place just as well as you go, probably.  \par
\par
But it's a status symbol.  So once we've got those clusters of people, we can start marketing to those people.  We can say, "people out there are buying the cameras for this reason.  Or that reason."  We can put out a coupon that says we've upgraded the camera with twice as many megapixels.  \par
\par
Amazon does this.  They find people who buy cameras and they also buy lenses -- oh, let me get a camera.  Here's a camera.  When you buy a professional camera, you buy a camera and a lens.  And then you usually buy a flash and bunch of other stuff.  Tripods.  Things like that.  \par
\par
So you have a cluster of people who will buy high end photography equipment because cellphones don't cut it.  You have people who buy bicycles, tires, diapers -- whatever it is.  I'll make my life easier.  I'll give them a stereotype.  It's a label I'm putting on the cluster so I can talk about them from now on.  \par
\par
I can talk about the people who buy my cameras who are technical experts and they are the technophiles.  Not technophobes.  Those are the opposite.  Technophiles love technology.  The early adopters.  For Amazon, we may have a cluster of people called photographers.  Bicyclists.  And a bunch of people who buy baby food and diapers, we call them parents.  \par
\par
So those are used in recommender algorithms.  Who to send a coupon to or if somebody buys a camera we can recommend and suggest things they may want to buy.  So that's a label.  It's a word we use to talk about them.  \par
\par
But we can't talk to the computer in terms of words.  We need a mathematical model.  The mathematical model we call a prototype.  Prototype.  \par
\par
That's like how many diapers do people buy?  And what size do they buy?  OK?  \par
\par
So imagine there's a group of parents.  I find a group of parents.  They weren't parents before but it's a subgroup of parents now buying a bunch of size 1 diapers.  What does that tell me?  They never bought them before but they are now.  And size 1.  \par
\par
Michael is right.  They're new parents.  Now, what is that going to tell me about a year from now?  What prediction can we make about them?  What size diapers are they going to need?  \par
\par
None of you have kids.  You won't know.  Maybe some of the interpreters know the answer.  Right?  But, the point is that the diapers go up in size.  Right?  It's not going to be a size 1 for long.  When babies are born, they fit in the palm of your hand.  They're dinky.  Thank goodness.  You feed them milk and within a couple weeks they're twice the size.  They grow at a tremendous rate.  \par
\par
Life is a lot better for moms if it works out that way.  Brian is right.  They go up in size very quickly.  So a prototype is a mathematical model that helps me describe those parents or each cluster.  \par
\par
We're going to talk about agglomerative clustering today.  Divisive is the opposite.  We split them apart.  And then there's guess and check.  We'll use guess and check for \i\f1 k\i0 -\f0 means clustering later on.  OK?  \par
\par
Good!  \par
\par
Let's go into it.  \par
\par
So, we've got these basic 10 algorithms.  We could just throw everything in an artificial neural network.  Why is it important to know the top 10 algorithms?  The answer to that is, if you invent a new classification or clustering algorithm, once you have the top 10 around, you can then compare this knowledge about how these other things work gives you a common basis of comparison.  \par
\par
The other reason to know it and not just throw everything in an artificial network is if you do that, you'll find it takes 3 weeks to train and it doesn't do much better than some of the basic ones.  If it does, you don't know how much better.  You don't know.  \par
\par
But today we're talking about about hierarchical agglomerative cluster.  \par
\par
When we start clustering, I don't know which ones are buying cameras because they're pink, purple, or black.  Whatever.  When I bought a camera, I bought this camera because it fit in my backpack but it was also not black.  If I put a black camera in my backpack, I don't find it.  It gets lost.  I couldn't get it in my favorite color.  You get to figure that out later on.  \par
\par
Simple segmentation.  That's forming histograms.  That's not right.  \par
\par
Oh boy, I have to keep going.  There's a standard method for doing data science.  And you should know that.  It's called CRISP-DM.  \par
\par
Why do we care?  It tells us what we should be thinking about.  Ethics.  Should I be releasing this data?  Or collecting it in the first place?  Business understanding.  Am I going to make money?  Why am I doing this?  Will I learn?  \par
\par
Data understanding.  This is clustering.  And then we got into the data preparation person.  OK.  \par
\par
Raise your hand if you are a cook.  Raise your hand if you -- do you know how to make soup?  OK.  We have a couple people who know how.  That's good.  I see a bunch more people who know how to make soup.  Great.  \par
\par
Guys, you should all learn how to cook.  It's a good life skill.  Guys, I mean everybody.  OK?  \par
\par
So we go cook.  When I make soup, I put the leftovers in a pot and add some vegetable stock and bring it to a boil and stir it.  Then I take a spoonful of the stuff out, let it cool, and then I sip it.  \par
\par
But let it cool down first.  I say, "Hmm . . . needs more chili powder."  Because I like that in soup.  Or more onions or salt or something.  And then I change the flavoring.  Maybe I add other vegetables and change the flavoring and keep iterating until it's good enough.  All these processes iterate.  So you can cycle back on themselves.  \par
\par
So you go to a job interview.  Somebody in the job interview says, "Hey, so we want to give you data on seismic activity."  And you've never played with that before but you want to work for them because they do cool stuff.  \par
\par
You tell them you've never done things with seismic activity.  But there's a standard process -- the CRISP-DM -- the standard process for datamining.  I would get domain knowledge and keep working on it.  So you can still work on data you don't know about.  It's good to know.  \par
\par
It's a good answer to have in your back pocket.  In case somebody asks you to work on data you don't know before.  We'll form some hierarchies of clusters.  \par
\par
These are similar, we put these together and we'll put those together into one cluster and then you keep going until they are bigger and bigger.  Not far way I have a bunch of computer equipment and I have to put it into boxes.  I have fans and they're like 4 different sizes.  So I'm putting those together in one box called computer fans.  And I have power supplies.  \par
\par
I'm organizing my life based on function.  But it could be based on size.  I'm doing some sort of bottom up agglomeration.  I could have put everything in one big box and divide it later on.  That would be top down.  \par
\par
Historically when we used to do agglomeration, the historical approach was to take your data, look at all of it, look through all of it and find -- I have person 1, 2, 3, and 4.  Of all of these, these two are the closest together.  So they become clustered together into one.  This forms a new seed.  \par
\par
Using that seed, we now grow the seed out.  When I use this technique, I just have to keep track of what's the next point closest to the cluster?  Number 4.  That gets merged in.  Now I have a bigger cluster.  A bigger seed.  This is the traditional hierarchical clustering.  The next is over here and that's merged in.  OK?  \par
\par
Alright.  That's traditionally how it's done because it didn't use much memory.  Oh!  Look!  There's a dendrogram.  A dendrogram shows as merging goes along it shows how far apart are the points?  This height here is how far apart those two clusters were.  .2 and .3 were initially in their own clusters.  They're now merged together at this distance away and we replace them with a new cluster center.  \par
\par
Now I have a distance from here to there.  Oh!  Distance 4.  So this would be the distance up that 4 is.  And then gets a new center here.  \par
\par
And then we keep going as you go along you say now you'll merge this together.  With this here, the distance here is that one.  So that'd be this distance here.  See how that works?  I can look at this diagram and see which things got merged in at what time.  \par
\par
OK.  \par
\par
That's the traditional way back when memory was scarce.  We don't need to do it that way.  So I have my data here and I'll form a new set of clusters.  This time I search around and find that .3 and .4 are closest together.  So I form this cluster here.  They get merged together over here.  Boink.  There's some distance there.  \par
\par
Then I see 1 and 2 need to get merged together.  Some other distance metric, maybe it's the L\sub 1 \nosupersub instead of L\sub 2\nosupersub .  So these get merged together and we finally merge them all together.  This new modern hierarchical clustering technique requires more memory because I have more clusters to keep track of.  \par
\par
This is the way it's done these days.  I guess.  Actually, now that I think about it, I don't know why we can't use traditional clustering for big data.  I have to think about that.  \par
\par
Anyway, this is the one you'll use.  This alternative hierarchical clustering.  The new technique.  \par
\par
Any questions?  \par
\par
Student:   Is it just a measure of how far apart the data is?  Just a changing of that measurement is how you get from additional to alternative?  \par
\par
Professor:  When I'm doing the traditional, I have one cluster center that grows and then I update the cluster center and it grows, etc.  I think of a raindrop.  Raindrops start by tiny water molecules that bump around into each other and they agglomerate.  As it falls through the water vapor it adds on other things.  That one raindrop agglomerates more water molecules as it falls through the atmosphere.  It adds things on.  \par
\par
This alternative technique says everybody is at their own place and they find each other and merge together.  Up on top, I have one cluster center that moves from here to there and then maybe out to here.  That one keeps growing.  Here I have this center of this one, the center of this one and also we have one up here.  \par
\par
There's a lot more record keeping but that's how it's done now.  Because I want to find subclusters easily.  Does that answer the question?  Or completely confuse you?  \par
\par
Student:   That answers it.  Thank you.  \par
\par
Professor:  OK.  Memory is cheap.  We do it this way now.  \par
\par
[Recording stopped.]  \par
\par
[Recording in progress.]  \par
\par
I'm just trying to break things up a bit.  Let's keep going.  There's a lot here.  It's huge.  Huge slide deck.  A clustering is a particular way of breaking up your data.  As we'll see there's many ways to break up your data.  You could have the same data broken up several different ways.  \par
\par
So we've seen this slide before.  What's wrong with it?  It's true.  The intracluster distance is labelled.  But there's problems with it.  And the problem is they're classified.  They're given labels already.  Yes.  Correct.  Thank you.  \par
\par
But the word intracluster distance is correct and intercluster distance is correct.  \par
\par
People may say we have clusters and we want high cohesion.  Cohesion measures how closely related things are and we'll talk about the sum of the square error soon.  We want good cluster cohesion.  Cluster separation is like the intercluster differences.  \par
\par
One of the best measures of cluster cohesion, the most common one, is called the sum of the squared error.  I have 3 data points in the cluster on the left.  I have two data points in the cluster on the right.  The cluster on the left we should call C\sub 1\nosupersub .  \par
\par
Cluster 1.  And then this one cluster number 2.  So, going through the math, here's what we do: for the cluster is 1 to \i\f1 k.\i0\f0   \i\f1 k\i0\f0  is 2.  You want to compute the distance to all the point that are all parts of the member.  For all of the points that are inside cluster 1, that's 1, 2, and 3 here, find the distance to the mean of that cluster.  The center of the cluster.  That means we're going to need a mean.  \par
\par
I don't know, I could put a red \i\f1 x\i0\f0  out here or something.  Oh, there we go.  Cool.  Now I have center 1 and center 2.  Those are the means.  Good.  \par
\par
Now that I have those, I need to find the distances from \i\f1 x\sub\i0 1\nosupersub\f0  to its mean.  Let me use a straight line for this.  Cluster 1 to -- there, that's a distance.  I square that distance and add it.  Then I take the distance from 2 to 1 and add it up.  Cluster 3 and I find the distance and square it and add it up.  \par
\par
We will use this formula and this idea, the sum of the squared errors, a lot.  \par
\par
Then I go to the next cluster.  I go to cluster 2.  Cluster \i\f1 i\i0\f0  is equal to 2.  So I find the mean, there it is, and the distance to all the \i\f1 x\i0\f0 s that are a member of that cluster.  There's \i\f1 x\sub\i0 4\nosupersub\i  \i0\f0 and \i\f1 x\sub\i0 5\nosupersub .\i  \i0\f0  Got it.  Add them up, square it, that's the sum of the squared error.  OK?  \par
\par
So you might look at it this way.  We might look at it this way.  You pick.  \par
\par
Questions?  Could you go off and recompute that if you wanted to?  \par
\par
I see a raised hand, did somebody leave that up?  \par
\par
Questions?  Comments?  OK.  \par
\par
So that's the formula.  Here's the verbiage in case you want to read along.  Another method is called the silhouette coefficient.  So I have a bunch of data points in the cluster on the left and a bunch of data points in the cluster on the right.  So I'm going to compute the silhouette coefficient.  For the cluster on the left I compute the average distance to all the points in this cluster.  \par
\par
That's called \i\f1 a\i0\f0 , the average distance to everything in there.  Then I also compute the minimum distance between any one point in this cluster and any one point in that cluster.  The single linkage distance.  We'll see that soon.  Using that single linkage between this cluster and that cluster, the shortest distance between any point on the left and any point on the right is \i\f1 b.\i0\f0   So I divide \i\f1 a\i0\f0  by \i\f1 b\i0\f0  and then subtract 1.  I say \f1 1\i  \i0\f3\u8722?\i\f1\lang1033  a \i0\f0 divided by \i\f1 b.\i0\f0   \par
\par
If these two clusters are far from each other, \i\f1 b\i0\f0  is really high.  Imagine that \i\f1 b\i0\f0  goes to infinity.  As it goes to infinity, \i\f1 a\i0\f0  divided by \i\f1 b\i0\f0  goes to 0.  So the silhouette coefficient goes to 1.  That's a good thing.  If \i\f1 b\i0\f0  is relatively close, just a little more than this distance, then \i\f1 a\i0\f0  divided by \i\f1 b\i0\f0  is nearly 1.  So that'll be something that's nearly 0.  \par
\par
That's not a good thing.  The silhouette coefficient shows us how dispersed the clusters are.  It combines the ideas of cohesion and separation.  \par
\par
So you're running an algorithm and it says here's your clusters and the silhouette coefficient is 0.24.  You get to look at that and interpret that.  You change the distance metric, rerun the algorithm, and it comes back with .43.  Which is better?  You have to know that.  \par
\par
OK.  Partitional clustering.  Let's keep going.  \par
\par
We talked about crisp versus fuzzy before.  It's possible to have fuzzy clusters.  When did we talk about this?  What else is crisp versus fuzzy?  Remember when we talked about mixture models?  People who were -- they could be crisp.  Either Republican or Democratic.  When we get to higher, bigger clusters they could be \f1\'bd\i  \i0\f0 democratic -- like a family unit could be mostly Democratic but a few Republicans in there.  Or something.  It can get fuzzy.  \par
\par
So there's a mixture model to people, but we can also have fuzzy clustering techniques.  Where a person is \super\f1 1\nosupersub\f2\u8260?\sub\f1 5\nosupersub\i  \i0\f0 a member of that group and \super\f1 4\nosupersub\f2\u8260?\sub\f1 5\nosupersub\i  \i0\f0 a member of this group.  They used to be a bicyclist for the longest time and then they started buy diapers so they became the cluster of new parents.  So they're \super\f1 4\nosupersub\f2\u8260?\sub\f1 5\nosupersub\i  \i0\f0 parent now and \super\f1 1\nosupersub\f2\u8260?\sub\f1 5\nosupersub\i  \i0\f0 bicyclist.  \par
\par
More jargon than you ever care about.  \par
\par
Well-separated clusters.  Just the idea of being separated.  \par
\par
Center-based clusters.  Here's center-based.  That means I have a center and some distance from the center.  This is one of the most common forms of clustering we use.  Look at this, this one on the right is elliptical.  Look at that!  So this one here is elliptical.  Any time I have ellipses going on, what distance metric am I using?  \par
\par
It's the Mahalanobis distance.  It's a clue when I see something like that.  The Mahalanobis distance is in use.  Contiguous and for practical purposes, you may need contiguous clusters.  We see 3 clusters on the left that are all contiguous.  \par
\par
Those clusters are all bus routes.  We have a certain point here and another point there and when we go plan a bus route, the bus has to travel in a contiguous path.  Amazon deliveries have to be contiguous.  Here's another one that's weird where you have a strange shape that's contiguous but connected by a little line.  \par
\par
Contiguous is important for route planning and some other issues.  Districts.  Congressional districts must be contiguous.  Here's a map of the City of Rochester.  What you can see is it has some strange shapes to it.  Oh, here's Ontario Beach Park here.  See that?  This is contiguous all the way up the Genesee river to the beach there.  \par
\par
Oh, here's this other weird thing.  Watch this.  \par
\par
Here, right up in here, is a little lot of land that's about 10ft wide and it goes from the city all the way up to the beach.  I don't even know what the name of this beach is.  But it's up there.  Because the city wanted to own that beach.  Right?  It has to be contiguous.  West Irondequoit is contiguous and so is East but they're separated by the city.  A municipality must be contiguous.  There's some practical reasons for being contiguous as well.  \par
\par
The city of Rochester owns this thing here but we won't talk about that.  OK.  Keep going.  \par
\par
Density-based.  This is the author's density based thing.  This is a lousy image.  Let's try this one.  In this image, we have a background density.  The points are generally dispersed at some range from each other.  But in some areas, you can see that they get a higher density.  \par
\par
So we could use Parson estimation to show there's a higher density in these locations and identify these clusters.  There's probably 3 clusters there.  This is density-based cluster.  We'd use the DB scan algorithm.  Density-based scan.  Used to find clusters in regions.  \par
\par
Oh, there's concept based clustering.  You're looking for rectangles, you map that concept.  Looking for donuts.  I don't know why you would be, but you can have clusters that overlap.  It's in the book, I had to cover it.  \par
\par
Objective based clusters.  You want to maximize or minimize some function.  We did this one.  Which one did we do?  With objective function did we maximize?  Michael Barton is on the ball.  Yes, Otsu's method.  Correct.  \par
\par
And when did we do?  We wanted to maximize or minimize something about the average variants.  Mentioned variants.  Did we want that high or low?  Right.  Good.  Low.  \par
\par
We wanted to be low.  So we've got less variance on average within each cluster we've got.  \par
\par
Linkage.  \par
\par
[Recording stopped.]  \par
\par
[Recording in progress.]  \par
\par
We mentioned linkage before.  I talked about when the silhouette coefficient -- the minimum distance between any one point in one cluster and any other point in another cluster -- I could have used another point in that linkage.  Linkage is used when agglomerating things.  I have more than one point in the cluster and I have to pick the points to measure between.  This is easiest shown.  \par
\par
I have a blue cluster and a red cluster.  I labelled them so you can see one cluster A and one cluster B.  I've already formed the clustering.  If I find the shortest distance between one cluster and the other, that's the single linkage cluster.  That's important for route planning.  \par
\par
If we had the shortest distance, you can also have the farthest distance.  This is the complete linkage.  This gives us good coverage.  \par
\par
You could do that if you want really circular clusters instead of long, skinny clusters.  The one that's most commonly used is the central linkage.  It's dirt easy.  The center of one center to the center of another.  There's also the average linkage.  Nobody in their right mind would ever do this.  In the average linkage, you compute the average of all possible connections.  \par
\par
It's a time consuming mess.  Here we have 49 links.  You have to find them all and then find the average.  What a mess.  At some point I actually computed it just for the fun of it.  It's 5.7.  But, phew!  Don't do this.  I've never see any other application outside of academia that does this.  Practically -- this is not easy to do.  \par
\par
Yeah, it's like bad run time.  \par
\par
OK.  Prototype.  Mathematical model.  We talked about this.  It's a model that represents the center and the extent of the cluster.  I've actually talked about all this before.  I just wanted to have the algorithm here.  I gave some pseudo code for doing the homework so I could talk about that.  \par
\par
You have design decisions to make.  You'll select the most relevant attributes to use.  You have the distance metric.  You have to pick the prototype.  And the linkage method between them.  Those are choices you make.  \par
\par
We're not used to using design decisions.  Usually the professor gives you a dosage decision and you have to use whatever the professor does.  Professors do that so they can grade the homework.  They're mean that way.  I'll let you play with things.  I'll have to make some decisions to make sure it works but in general you'll go through the process.  \par
\par
So we talked about dendrograms and how they form.  Here's a dendrogram.  I want to talk about how to actually break these things apart, in a second.  One of the problems is that clusters are ambiguous.  You could say there's two clusters there, or maybe there's two or 4 or 6 -- depends on how you look at it.  So the dendrogram helps us break them apart and understand what's going on.  \par
\par
The problem with the ambiguity is that it's like Anakin Skywalker.  Luke says to Obi-Wan, "I thought you said my father was dead!"  \par
\par
Obi-Wan says, "Many of the truths you cling to depend on your point of view."  It depends on your usage and the interpretation.  Is the infinity symbol on the infinity quad an infinity symbol or a 0?  Depends on how you look at the sculpture.  \par
\par
You're all fairly young compared to me.  Have any of you see the infinity sculpture rotating?  \par
\par
Yes?  OK.  I haven't seen it rotate in a really long time.  I think that the person in charge of turning it off for the winter retired.  And then never turned it back on.  So it depends on your point of view.  \par
\par
Measures of centrality.  Mean, median, mode.  The centroid is like the average.  And then there's the medoid.  The centroid is like the center of mass.  The center of mass is not necessarily a member of your data.  I have 4 data points here, I find the center of mass, it's somewhere not in my data.  So that's not representative of any data point I own.  \par
\par
The medoid is the real data point closest to the center of mass.  In theory, it doesn't matter how you cluster.  I'll skip over this.  Agglomerative design decisions -- we covered this.  This is redundant.  \par
\par
Let's look at how to read some dendrograms.  Almost everybody here would say, based on the 2D representation of the data, and more dimensions we can't see, here we see 3 clusters.  When we form a dendrogram there's a whole bunch of things at the bottom we can't see.  \par
\par
All the stuff down at the bottom cannot easily be seen.  To read it, what you do is this: you take each one of these values here, and you bring them over to this \i\f1 y\i0\f0 -axis.  You find a \i\f1 y\i0\f0  intercept.  Take each one of these and pull them apart.  \par
\par
There's something that's referred to as the natural clustering.  It's a convention somebody came up with.  The natural clustering for this dendrogram, we look at the left hand side and there's a lot of them that are really small and together.  I won't pay attention to those.  From here down it's not exciting.  \par
\par
But, from here to here is the biggest single jump.  Looking up the \i\f1 y\i0\f0 -axis I find the biggest single jump.  That's it.  Right there.  And that's the one I decide to split.  So I split this one in half.  I draw a line horizontally across my dendrogram.  When I do that, how many things do I cross?  I cross a point here, a line here, and another line here.  \par
\par
Under the covers there are 3 clusters.  So, on the left I've got this cluster.  And I could actually go in and count how many cluster values there are.  And then I have this one here.  And then I have this one here -- oh.  So those are the ones you can count up.  \par
\par
Can you read?  Look at the right hand side.  How many data points are on the right hand cluster?  \par
\par
I see 12.  \par
\par
Do we agree?  Maybe 13?  11 or 13 depending on how small that gets on your phone depending on how you're looking at it.  You form these lines for all the dendrograms.  If you have a piece of paper and you're taking an exam and it's written on a piece of paper, who cares?  You own that exam!  \par
\par
So then you take it, draw those lines -- I literally drew them with a ruler.  It's fun to do.  You take the whole paper and turn it sideways and look for the biggest single jump.  But with COVID protocols we'll do things differently.  \par
\par
I've been mentioning path planning as we go along.  It's an important use for agglomerative cluster.  Let's agglomerate airlines.  Suppose you want to decide which cities should be hubs, or which will be a data hub and you're running a fiber optic cable for your networks.  I've collected the distance between all these cities.  Boston to New York, District of Columbia, Seattle, San Francisco and Denver.  Notice it's symmetrical.  The distance from Boston to New York is the same New York to Boston.  \par
\par
Now we merge them and I'm using single linkage.  Now I have to update a bunch of distances.  OK.  I don't want to have to recompute everything every time.  I'll just update the ones I need to.  So now I go look for the lowest number.  233, that's DC to Boston.  So DC, Boston and New York merge together.  That's one hub center area for my airlines.  \par
\par
And then I have to recompute all of these distances.  And I look over the entire thing to find the shortest distance.  It's from Los Angeles to San Francisco.  There's a West Coast hub.  And then Chicago to DC becomes important.  So then you merge together all these, recompute a bunch of distances and find the minimum.  \par
\par
Oh, San Francisco and Los Angeles merge to Seattle at 880 miles away.  And on and on.  \par
\par
In reality, sometimes distances are not symmetrical.  Here's a bunch of distances and you'll notice the distance from Denver -- let's do LAX to Boston.  That distance is 325.  But the distance from Boston -- up here at the top -- the distance from Boston down to LAX is 370.  \par
\par
But the distance from LAX to Boston is only 325.  What's going on here?  Why is this not symmetrical?  What distance metric am I using here?  \par
\par
OK.  I'll tell you.  This is supposed to be symmetrical if they were nice distances.  But these distances are not.  What you're looking at -- yeah, they're not symmetrical.  Bummer.  What you're looking at is the flight times in minutes.  And when an airplane is in the air, it's burning jet fuel and somebody somewhere knows how expensive it is to burn that fuel.  \par
\par
So they want to minimize the time of an airplane in the air.  And the reason that they're not symmetrical is that -- the country is not symmetrical.  The distance between them is symmetrical on the Earth but you're not flying the you Earth.  You're flying through the air and there's this thing called the jet stream.  \par
\par
If I'm flying from Dallas -- I did do that once.  When I flew from Dallas, I flew from Dallas on up to the jet stream here.  And then they flew across to Rochester New York.  Literally it flew up -- they went way out of their way.  \par
\par
From then on they could just coast.  Not only that, but while they were flying -- they didn't really coast.  They flew but it took much less fuel to keep going that quickly.  \par
\par
So, yeah.  Jet streams.  OK.  \par
\par
This is something companies really think about.  \par
\par
OK.  I think that's about the time.  The whole point is, there's one way streets in this world and you have to think about them.  They'll actually happen.  \par
\par
Everything else here is like repetitious.  It's redundant.  I talked about jigsaw puzzles because you agglomerate based on features and they change as you go through.  We used to do a workshop a jigsaw puzzle doing to show you.  But we have COVID and I can't get everybody together.  \par
\par
I thought about trying to have everybody buy the same puzzle but haven't figured it out.  If anybody can invent a computer game where everybody can do the same puzzle together, I'm not at that yet.  \par
\par
Yeah!  We need a tabletop simulator or something.  If you know the answer to that, that could be your capstone project.  Or your senior project.  \par
\par
[Professor Laughing.] \par
\par
Student:   It was literally a Google away.  It's a multiplayer jigsaw game and people are playing.  \par
\par
Professor:  I have to go learn about that.  Very good.  Jigsawpuzzles.io.  Very good.  That's it.  Great.  Thanks.  \par
\par
All good.  Carson, what club uses that?  \par
\par
Student:   Circle K.  We use it for fellowship activities.  \par
\par
Professor:  OK.  Very good.  I'm going to say goodbye because we're at the end of our time and people have to go to next class.  Thank you all!  Talk to you later on.  Watch for activities.  Watch for announcements.  \par
\par
Student:   Do we have class on Tuesday?  \par
\par
Professor:  No!  It's a day of rest.  But you'll have a quiz probably on Sunday.  \par
\par
Student:   Sounds good.  \par
\par
Student:   Do you know when the next homework will be released?  \par
\par
Professor:  I have to write it yet.  \par
\par
Student:   No homework this week?  \par
\par
Professor:  I'm going to try and invent something new.  I want you to brush up on terminology and understand what you're doing.  I'll probably release it soon but it'll be due the following Sunday.  If I released it on Friday and it was due Sunday that would be mean.  I'm not that professor.  \par
\par
Student:   I appreciate that.  \par
\par
Professor:  OK.  See ya!  \par
}
 