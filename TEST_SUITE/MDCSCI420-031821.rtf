{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1033{\fonttbl{\f0\fnil\fcharset0 Georgia;}{\f1\fnil\fcharset0 Times New Roman;}{\f2\fnil Times New Roman;}}
{\*\generator Riched20 10.0.19041}\viewkind4\uc1 
\pard\tx440\tx880\tx1320\tx1760\tx2200\tx2640\tx3080\tx3520\tx3960\tx4400\tx4840\tx5280\tx5720\tx6160\tx6600\tx7040\tx7480\tx7920\tx8360\tx8800\tx9240\f0\fs22 Principles of Datamining\par
Professor Kinsman\par
CSCI-420-02\par
March 18, 2021 \par
\par
Professor:  Greetings.  Let us begin.  \par
\par
Let's see . . . 420 . . . I think we have our initial grades in for the midterms.  I think.  I haven't triple checked with the graders.  Work is still finishing off on some of them.  \par
\par
OK so where am I?  \par
\par
We need to do a project.  Because RIT wants everybody to do a project.  \par
\par
Student:   Excuse me.  Regarding the midterm, on Saturday you sent an email halfway through the test.  I was taking the test and I didn't see it.  I had some issues with the images not displaying.  I sent you a reply stating that but I didn't get a reply.  So I don't know if my midterm grade is taking that into account.  \par
\par
Professor:  I can't see who is talking.  \par
\par
Student:   Esteban.  \par
\par
Professor:  Send me another email and keep me in there.  I do realize that some things went wrong.  Yes.  For some reason Zoom is telling me that the interpreters are talking.  I don't know why.  \par
\par
Anyway. . . . crap happens.  When we look at your exam we should be able to see there were issues there.  \par
\par
I apologize for the issues.  Technical issues.  It's not as bad as it was last semester.  We went live with the midterm and there were lots of things messed up.  Missing images was not the only thing wrong!  There were cases where there were the wrong images.  And yeah.  It was yucky.  That's why we'll compensate appropriately.  Make sure I know who you are.  \par
\par
For some reason I can't see who is talking still.  \par
\par
Will you see a key for the questions like you did for the quiz?  \par
\par
Uhh . . . the problem is Kevin that your quiz was drawn at random from a pool of other questions.  So your questions are different than anybody else's question.  Some people by accident got the same question a couple of different times.  Which, let's hope they understood the answer to that question.  \par
\par
Just a second here.  We have another issue.  \par
\par
Let us begin again!  \par
\par
OK.  So I want to talk about the project.  And if you hated having to clean the data for the agglomerative homework, you'll hate this even worse.  So it has been a year since COVID-19 has been upon us.  \par
\par
I would guess that there are going to be fewer car accidents on the road.  Because there are fewer cars on the road.  Because fewer people are driving on the road.  \par
\par
Now, other people, Jonathan might guess the opposite.  Jonathan might say there are fewer people on the road and so therefore those people are driving recklessly and there will be more accidents.  We can form a hypothesis one way or another.  We don't know exactly what happens.  \par
\par
So here's the task for the project: \par
\par
You're going to go to the New York City open access database.  They record all the collisions since sometime in 2014.  Oh, it's made public May 7th 2014.  So there's this data out there.  They have visualizations of their own that sometimes work and sometimes don't.  I tried to run it lately but it didn't work.  \par
\par
You can download the data.  You just click on export and you'll download the data.  Within New York City there are 5 townships.  They call them boroughs.  Another one begins with M.  Manhattan!  That's it.  \par
\par
So they have accidents.  Car accidents.  Accidents involving car to car, car to bicycle, whatever.  They're all recorded.  And the data is out there.  The truth is out there!  So your job is to go form a hypothesis between you and a partner.  Two partners for 420.  \par
\par
Form a hypothesis, validate it and see if you can prove it to me.  Are there more accidents or fewer accidents?  Initially you want to some quick, easy EDA.  Exploratory data analysis.  You want to quickly look at the data, make an initial hypothesis and see how you'd tear it apart and prove it.  \par
\par
For example -- oh.  As part of this, you also get the longitude and latitude.  Bear with me.  I'm changing my formatting.  Because New York City is where it is, we can pretend that New York City -- that longitude and latitude are perpendicular.  They're not really.  We're using a linear approximation in the Taylor Series expansion.  First order approximation.  We're assuming all of the data is linear.  \par
\par
So we can break the region up into grids.  Plus or minus a half of a degree of longitude and latitude and you can compute within a given region how many accidents were there?  You can add them up for this grid and for this grid and this grid and you can get an estimate and that would be doing some density estimation on the grids.  \par
\par
Now I think that the data might be most susceptible to changes in August.  In August we have weirdos -- I'm sorry, tourists -- that show up in New York City and they don't know their way around and therefore are more likely to have accidents.  I would be a weirdo if I went to New York City.  My general plan for New York City is to avoid going there.  \par
\par
I know some of you live there.  I don't mean to offend you but it's not my neighborhood.  \par
\par
Here's another way you can do density estimation.  You can say I have an accident here, an accident here, there, over here, over here . . . OK?  And so given that, now I could say well let's look in a region around each point.  So for any given \i\f1 x\i0 ,\i y \i0\f0 intersection, we can estimate the number of data of accidents within this region.  \par
\par
Oh!  Looking at this I have 6.  Within a nearby -- this little intersection -- I would say there's about 6 accidents there roughly.  For some time slice of your data.  You don't want to do it forever because cumulatively that would be complicated.  \par
\par
It's good for you to go and do a time slice.  I would say August might be a good time.  It could be February because there's variation when the weather is yucky.  Maybe it's Christmas.  We don't know.  You get to do some data analysis and see what you can find out.  \par
\par
How did COVID, the overall question is, how did COVID impact the New York City crash map?  Accidents.  Did it make them go up or down?  What?  \par
\par
So I can go find -- oh and so if I go build these densities everywhere we want to make sure we don't miss anything.  So you have to build a radius that's big enough that it comes up with a good density estimate and you get to pick the radius.  \par
\par
And I would expect you would generate a heatmap.  Basically I'm getting a density estimate here, here, and here . . . and so I get a surface plot.  \par
\par
And you can use any packages you want for the project.  The goal is get to done.  Don't take forever.  But you still have to do data cleaning.  You can't compare apples and apples necessarily.  One year we may have more pedestrian accidents than car accidents.  There may be more -- whatever.  You know what I'm saying.  There may be more accidents with bicycles.  They have bicycle couriers because it's easier to get around on a bicycle than taking a taxi.  That's what I've heard.  \par
\par
So there may be a lot of bicycle accidents and you want to focus on those.  What is the best time of year to bicycle around New York City?  That's another question you may ask.  Has that changed with the aspects of COVID?  \par
\par
I know some people have looked at this and they say the most likely time to have a crash in New York City is on a Friday.  Because people are reckless.  Or careless.  They get up and they drive off on Friday.  They leave their jobs early and have accidents.  \par
\par
So that used to be true.  Is that still true?  Has it changed?  I don't know!  You get to figure it out.  \par
\par
I have not yet figured out myself how to assign groups to the class.  I will eventually.  Things will go up and down and I'll send you an email when things happen that are good.  OK?  Got it.  Good.  \par
\par
Think about this.  Have a look at this.  Form partners.  I want 2 people at least working on it.  And one person doing the quality assurance and writeup work.  I need to make sure there's somebody there who is making sure you guys are not making stuff up and that this really does work.  \par
\par
There can be no questions.  We will have more questions later on.  \par
\par
[Recording stopped.]  \par
\par
Now!  Part of the reason we have a project is some people did really well on the midterm and the question is, oh, that's good you're good in theory but are you really good in practice?  So the project is to give you a chance to practice.  \par
\par
What will we talk about today?  We have to know how to validate our data.  So we'll talk about N-fold cross validation today.  Bear with me.  \par
\par
[Recording in progress.]  \par
\par
This section of the lecture is about what we call N-fold cross validation.  This addresses the problem that we don't know how good our classifier.  We're trying to guess between Assam and Bhutan.  A two class classifier.  But we don't know the best design decisions for my classifier.  \par
\par
I'll build a decision tree.  I don't know what depth to let my decision tree grow to.  We'll figure it out.  \par
\par
So I have a very limited amount of labelled data.  And I don't know the best parameters for my algorithm.  I don't know the best stopping depth for decision tree.  I don't know the minimum number of data in the node.  So if I get down to 5 data points in a node, I stop.  Or maybe 9.  Or 23.  I don't know what that number is.  \par
\par
Or, I might want to get down to a node and it's 95% pure.  And then I'll stop.  Because it's 95% pure and we can't get much better.  Stop.  \par
\par
OK.  \par
\par
Again, that labelled data is expensive.  It takes years to collect carefully calibrated correct data.  At National Geographic we had to send people into the Himalayas to keep track of the snow folk they saw.  They were risking hypothermia.  It's difficult to collect this data.  Even for speeding data it's hard.  The police officer has to risk getting shot by somebody.  It's dangerous to actually pull somebody over.  \par
\par
Much more dangerous than letting them drive by.  So the police officer asks them to pull over and says, "What were you thinking?"  And writes that down.  So that's the latent variable they're trying to find out.  \par
\par
Labelled data is expensive.  Unlabeled data is cheap.  We have lots of unlabeled data.  There's some exceptions to that.  Facebook hacked their whole problem by harvesting your images.  They had people labelled by the algorithm automatically.  So you provided the training to the algorithm.  \par
\par
I was always tagging the wrong thing for the longest time.  \par
\par
So we don't know given this labelled data, I want to know the best stopping depth or parameters for my decision tree.  \par
\par
For my artificial neural network, I want to know the best kinds of layers.  Should I have skip connections?  RNNs?  All of these issues come up.  Short term?  Long term memory?  And so on.  \par
\par
We can find the answers to some of these problems in part using cross-fold validation.  Here's how it goes.  \par
\par
Oh wait, this is a little too confusing.  I'm going to simplify our lives here a little bit.  I'm just going to ignore some of this for a second.  Let's pretend that we don't see any of these stuff here.  Keep it simple for now.  \par
\par
We divide the data into 3 sets.  At random.  Not completely random, but somewhat random.  3 sets.  Set 1, 2 and 3.  \par
\par
Now, I'm going to talk about 3-fold cross validation so I divide it into 3 sets.  OK.  When I do those I'm going to pick random data points out but I'll do it somewhat carefully.  I pick it out so that I have about balanced data in each set.  I want it to be close to 50/50 as I can.  Half Assam, half Bhutan.  \par
\par
So for stopping depths of 3, 4, 5, 10, I'm going to set the number of mistakes equal to 10.  Then I train a classifier using set 1 and set 2.  The combined total data that's in set 1 and set 2.  Then I evaluate the accuracy of the classifier using set 3.  \par
\par
So, for this particular part, I pretend that I don't know what is set 3.  I don't know what the labels are.  And I take all of the input attributes, I put them through my classifier, my decision tree or whatever, and I classify them.  Then I remember -- oh wait!  I actually know the answers.  I peel the label off hiding the answers and I look at the labels.  OK.  \par
\par
I've got those answers.  I see how many mistakes that made.  So I compute the number of mistakes I made overall.  Got it.  \par
\par
OK!  So now I repeat this process again for some other choices.  So now I'm going to train the classifier again using set 2 and set 3.  Then we evaluate the accuracy of that classifier using set 1.  Again, I find the number of mistakes that happened in set 1, I add them on to the number of mistakes there, and that gives me more mistakes I have to think about.  \par
\par
And then we repeat this one last time for the last set.  Here I train the classifier using set 1 and set 3.  I evaluate the accuracy using set 2.  Again, I have a certain number of mistakes.  I do this whole mess for depth equals 3.  I would then know how many mistakes do I get if I use a depth of 3 for my decision tree?  \par
\par
I repeat the whole shebang for a depth of 4.  And then a depth of 5.  And so on.  OK?  \par
\par
So what I'm doing when I do this -- and when I'm done with this, I know which of those depths gives me the fewest number of mistakes for my data.  \par
\par
So . . . \par
\par
Under the covers of all this, I'm assuming that one of those depths is the right ones to use.  I would think if the number of mistakes kept going down I would keep going and try a depth of 12.  So I use the depth with the minimum number of mistakes for my data.  \par
\par
And I could do this also for either decision trees or artificial neural networks.  But ANNs take a long time to train.  \par
\par
If I get to a point where I have really small data -- labelled data is hard to collect, remember?  If I have really small amounts of data, like I have 14 data points.  I have 14 point data and I want to do 14-fold cross validation.  This situation is called hold one out cross validation.  That means I have 14 data points.  I'll hold one out and build a classifier based on the others and I'll test to see if I classify this point in my hand correctly.  \par
\par
I put that one in, take the second one out and build a classifier based on everything in the set and then I look at the second one in my hand and see if that's classified correctly.  And put it in.  When I'm all done with this, I have held out each one of the data points.  How many decision trees do I build if I'm doing this?  \par
\par
Michael Barton wins the prize.  I'm building 14.  Exactly.  \par
\par
Good.  Remember that.  \par
\par
So here's an example.  I have data about playing.  It could be golf or tennis.  I changed it to tennis.  Who cares?  I don't play either one.  [Professor Laughing.] \par
\par
Alright!  So of my records, I hold out one third of them.  Again I'm trying to keep it balanced so there's a bunch of no's and yes's in there.  I build the classifiers based on the green and I test it based on the ones in darker colors.  \par
\par
Then I repeat.  I hold out the ones that are dark, pretend I can't see them, build the classifier based on the ones in green, and then I test them based on the ones in dark grey that are hidden.  This is 3-fold cross validation.  That's what we use.  \par
\par
We do it again.  \par
\par
The last time.  \par
\par
OK.  So . . . \par
\par
That gives me the accuracy for this data using 3-fold cross validation.  So in general, we take our data, divide it into N random sets.  For some parameters, 3, 4, 5, on up to 10, I set it to 0, then count and train the classifier for each of the folds I held out.  I train the classifier for all the data not in the held out set.  \par
\par
I evaluate that accuracy.  I add up the number of mistakes.  I do that for every single set I'm holding out.  And when I'm all done, now I remember what was the minimum number of mistakes?  What was the best parameter values?  OK?  \par
\par
OK.  \par
\par
And then I'll use the depth with a minimum number of mistakes.  \par
\par
Key point to keep in mind here: I'm using N-fold cross validation to find the best parameters for my classifier.  Or I could use it between two different classifiers.  \par
\par
So let's say I have some classifier and I'm going to let the complexity of the classifier go up.  As that complexity goes up, the error in my training data goes down.  Guaranteed.  The number of errors in the training has to go down as the complexity goes up.  If I use a deeper and deeper decision tree it'll work better and better.  \par
\par
But there's this other curve here.  \par
\par
And this curve is the error in the held out data.  This is the training error.  And this is the testing error.  The error in the held out data.  So it goes down and at some point the complexity of my decision tree gets too high.  We talked about this before.  \par
\par
I want to find the minimum amount of error in my testing error.  That's the place where I use for my -- I use this amount of complexity down here on the bottom axis.  I look up the complexity.  Maybe it's a depth of 7, whatever that is.  \par
\par
And that's the design choice that I use.  It could be that I get two points here that are very close to each other.  I get a 7 and an 8.  They're fairly close.  So I'll use 7 and not 8.  Why?  What principal of datamining am I using when I choose 7 over 8?  Yes.  Occams Razor.  Excellent.  \par
\par
So I'm using this to pick my best parameter choices.  How much complexity?  How pure does the node need to be before it becomes a leaf node?  How few data points do I have to have in my node before I stop and say I only have 3 data points I better make a decision?  OK?  \par
\par
I want to avoid overfitting and I also want to avoid underfitting.  Both are bad.  Got it.  We talked about this before.  \par
\par
Now, I have a pet peeve.  And that's that validation data in my mind is related to government contracts.  Or contracts with another company.  That's data you don't get to see.  If I'm building a classifier that's trying to look at a cell and tell whether it has cancer or not, I don't see the validation data.  I could actually build a computer vision system that will look at the cell and say, "oh, we think we see cancer in this cell."  \par
\par
And so based on that, and domain knowledge, etc., I would do that.  If I'm working on this for a company and they give me the validation data and say here's the data you need to get accurate, I'll readjust and keep tuning my system until it gets the validation data perfectly.  That's why you should be never shown the validation data.  The validation data is essentially a 3rd party check on you to make sure that you're doing things correctly.  \par
\par
And I've seen computer science computer vision experiments where they give students the validation data.  If a sense, you have been given the validation data in experiments.  You could retune your classifier until it works perfectly for the validation data.  \par
\par
Even if you don't know the answers to your validation data, you could guess the answers and then change your classifier.  \par
\par
Again, N-Fold cross validation doesn't get legal validation data.  The problem is that so many people can do data science now.  Anybody can pick up a computer and start playing with PsyKitLearn.  They will start calling the data validation data.  They'll call testing data validation data but it's not really the validation data.  \par
\par
It holds out data and that data that's held out is used to emulate future data you have never seen yet.  \par
\par
So . . . \par
\par
Which of the following would N-Fold cross validation help with?  Does it help decide using a decision tree and a random forest?  Yes.  Can it help you pick how many stumps you need in a random forest?  Yes.  \par
\par
It helps you decide the parameter settings for K nearest neighbor classifiers.  Yes.  We'll talk about that in a second.  \par
\par
It helps you decide which data is good or bad.  Not really exactly but it could be done.  You can certainly say which features should I use?  I try the first half and then the other half and combine them together until I find sets that work out really well.  \par
\par
It can do sensitivity analysis.  When I get down to about 7 layers, doesn't get much better than that.  Stop at 7.  \par
\par
It's used with convex optimization.  Absolutely.  You can use this idea to test your regularization design.  \par
\par
OK so what principal of datamining here in this class says . . . what principle says we need to do this?  \par
\par
[Professor Laughing.] \par
\par
Exactly!  No free lunch theorem says the only way to know the best parameter settings is to build it and test it.  \par
\par
Good.  Glad you're awake.  \par
\par
The other section -- I asked them questions, they just sit there looking at me like they're making dinner.  I don't know what's going on!  \par
\par
I have 100 data items.  I want to know if you should quantize the snow folk data.  I'll use hold one out cross validation.  How many different design decisions do I make?  How many different decision trees do I need to build?  \par
\par
Brian is way ahead of me!  300.  Does everybody get that?  For each parameter choice, and I'm holding one out, I'll build 100 different decision trees and then evaluate them.  \par
\par
So I modify my data by quantizing it to 1cm, 2cm -- first let's just do the 1cm.  Then I do the hold one out.  I have to hold each one of those 100 data items out.  I build 100 decision trees.  Then I do the same thing for 2cm and 5cm.  So overall I have 300 decision trees.  \par
\par
I'll use 10-fold cross validation.  How bad is it?  \par
\par
Michael, I'll get back to your question in a second.  \par
\par
Yes.  There's 30.  Alice, good for you.  On the ball.  \par
\par
Can't this get computationally intensive?  You bet!  This can take a long time.  If I'm doing N-fold cross validation, I test it on a small set of my data and then I give it the full set of my data and go to bed and wake the next morning and see the answer.  It's certainly easy to make it computationally complicated.  \par
\par
Especially if you're testing an artificial neural network that takes 3 and a half weeks to set up.  Yes, it can be bad.  \par
\par
So I have 1000 data elements and I want to know the depths of . . . you can figure this out later.  \par
\par
OK.  Yeah.  These are all good questions to know the answers to.  \par
\par
Good.  \par
\par
Another interesting question is, what is the best value for N?  In general, we use 10-fold cross validation.  There have been industry studies done a decade ago that say in general we think -- about 9 years ago they thought that 10-fold cross validation was the best -- 10 was a good choice to use.  It's a good rule of thumb.  Is it always the best?  I don't know.  I haven't done the studies myself so I'm hesitant to sign off on it.  That's just what I've been told.  \par
\par
[Recording stopped.]  \par
\par
Michael, any other questions?  No?  OK.  \par
\par
Oh, we're going to design airfoils.  This is going to be fun.  Woo-Hoo!  \par
\par
[Recording in progress.]  \par
\par
So, we have been beating around the bush about this K nearest neighbor thing.  And . . . the question is, what is K nearest neighbor?  How does this work?  What is K nearest neighbor?  Please, burn this into your mind.  K nearest neighbor is a classifier.  Every semester I have some student somewhere that confuses K nearest neighbor and K means.  K nearest neighbor is in red.  It's used for classification.  \par
\par
K-means is used for clustering.  Do not get that question wrong on the final exam.  It's a classification technique.  Not a clustering technique.  Got it.  \par
\par
This is a data-based decision classifier.  We look at the data we have and we say, oh, well it walks like a duck, talks like a duck, sounds like a duck, it's probably a duck!  Seems to be based on our measures on the features we've collected on the attributes we've collected.  \par
\par
We think it's probably a duck.  But it could be a goose.  We don't know.  OK?  \par
\par
Well, I have two data points here.  One is Assam and one is Bhutan.  Class A, class B.  It's a two point classifier right now.  Two class classifier.  I have this question mark here.  I don't know.  Is this closer to class A, the red class, or class B?  The blue class.  \par
\par
Michael says A.  Sure.  I believe that.  Yes.  Red.  We would classify this as red.  \par
\par
Simply because in this particular distance space, using the assumed Euclidean distance that's shown here, that point is drawn closer to the red one.  \par
\par
Alice, what about this one?  \par
\par
What do we have?  I know you're out there.  I think it's OK to pick on you.  Yes, exactly correct.  \par
\par
Oh!  Here is an interesting idea.  If I only have two data points, or, if I only have two centers for my classes, I could use a projection vector.  \par
\par
I'm going to make sure I don't do this too fast.  It gets weird.  Here's my projection vector.  It's drawn from one center to the other.  Now, I'm going to project the data onto that vector.  Again, this is math and some people lose it.  But to the point is, I'm going to take that data point and draw the perpendicular line to the vector.  \par
\par
And now I'm looking at this point.  Where does this perpendicular line to the vector intersect the vector?  Ah, it's closer to a red point than a blue point.  Because this point is closer to the red class than the blue, the point in question, that one way over here, would be classified as a red point.  \par
\par
So that changes to be red.  \par
\par
OK?  \par
\par
That's -- I just want you to remember how these all come together.  I want you to know the context.  How does this relate to other classifiers?  We can do this for several different points.  \par
\par
Here's another data point.  That gets projected on to the projection vector.  That gets projected to the blue side.  And so on.  And so forth.  \par
\par
Now I could actually put a threshold.  There's a threshold somewhere along the projection vector that says if you're beyond this threshold when I project it, you'll be classified as blue.  Otherwise you're classified as red.  Does that make sense?  \par
\par
So now, the question is, hey, I have questions all over the place.  I have all kinds of data points all through my feature space.  What would this decision boundary look like?  \par
\par
Joshua, what's the decision boundary going to look like?  \par
\par
OK.  Anyone else?  \par
\par
Oh!  Yes.  Aidan is right.  A vertical line right down the middle.  A vertical line.  Essentially everything that's to the left of this vertical line is closer to the red data point than it is to the blue data point.  Everything that's to the right of this decision boundary, is closer to the blue point than it is to the red point.  So everything on the left of this decision boundary would be classified red.  \par
\par
And everything to the right is blue.  \par
\par
So, what's important to me is that you know the difference between the projection vector that we project the data onto and make a decision versus the decision boundary.  Which is that defining boundary in feature space that says if you're to the left of this, you're one class.  If you're to the right, you're the other class.  OK?  \par
\par
Got it.  \par
\par
Good.  \par
\par
Oh!  How does this relate to the Mahalanobis distance?  Is the line always perpendicular in the Mahalanobis distance?  No.  It's not.  Correct.  You can have weird shapes in Mahalanobis distance space.  If you haven't seen the Mahalanobis distance video, it's being captioned now.  And will be put up.  Shortly.  \par
\par
The Zoom video has the automatic Zoom feature and so you can go with those Zoom features.  I went through and changed every place where it said "Mahal Noblese" which sounds christmas-y to Mahalanobis.  So it's better than automatic.  But not perfect.  \par
\par
Oh, what about 3 classes?  We can do this.  We can extend K-nearest neighbor to work for 3 classes.  It gets more complicated, but you get the idea.  \par
\par
One of the biggest problems is we have to find the nearest neighbor or neighbors.  We also need to find the nearest neighbor for agglomerative hierarchical clustering.  We have to find the nearest neighbors so that's a problem that's endemic.  \par
\par
We'll talk about this in a second.  How do you make that more efficient?  In general this is the k-nearest neighbor classifier.  I find the K other data points closest to it, I classify the unclassified data as the most likely class as the k-nearest neighbors.  I almost always use an odd value of K so I never have a tie.  \par
\par
Especially for the two class situation.  If I have three nearest neighbors, 2 out of 3 wins.  And this is called a memo-ization technique.  We have to remember all of the data.  We have to see all the data we've had before and keep it in memory.  \par
\par
So here's a question . . . \par
\par
Oh!  I have K = 1.  Got it.  If I use that, I'm going to go find the nearest point there and it's red.  So this point would be classified as a red point.  \par
\par
But K equals 3 -- if I use that it would be now classified as a blue point.  K equals 5 -- uh-oh now I have 3 out of 5 that are in the red class and two in the blue class.  So for K = 5 now it's a red class.  \par
\par
Bummer.  \par
\par
So this gets us to an interesting question.  How do we choose K?  \par
\par
OK.  You're going to have set the sandwich down and type in an answer.  [Professor Laughing.] \par
\par
Yes!  OK?  We should all be able to answer this by now.  N-fold something or other.  You have to test it with N-fold cross validation.  It's the only way to know for sure.  \par
\par
Some people think that I just sit here and read the slides.  But, it's not true.  The reason that my words match the slides so much is because before I give the lecture I sit here and type up everything I want to say.  But sometimes I forget.  And here's an example.  What are the trade-offs between using K = 1, a small value of K, and a big value of K?  So write this down.  I forgot to put it in the slides.  \par
\par
When we have a small value of K, it will find and detect some variations in my data.  If I have a little fissure of my data that goes out some way and it's similar to something else, K = 1 will find the closest point to that and it will follow small variations in the data.  And it will also classify based on noise.  \par
\par
So if there's one data point that's wrong in there, K = 1 will find it and there will be a region in my data space that will be classified wrong.  \par
\par
On the other hand, for a large value of K, it will classify based on general trends in that area.  So that's the trade-off.  Do I want to follow the exact local trend or follow the biggest trend?  And so those are the trade-offs.  \par
\par
I think we've been over this like several different times.  Got it.  \par
\par
Now, I want to figure out the difference between an Assam and a Bhutan.  So I have this one person -- this one snow folk that shows up.  He or she has a bang length of a certain length and an eye color of a certain color.  I can look based on my 5 nearest neighbors and say this person is probably whatever the red class is.  Maybe Assam versus Bhutan.  But I can also modify my distance metric.  \par
\par
Because the length of your hair and color of your eyes are completely unrelated to each other.  So it's possible I want to scale these attributes so one has more or less importance.  I can rescale the \i\f1 x\i0\f0  dimension so it's \super\f1 4\nosupersub\f2\u8260?\sub\f1 3\nosupersub\i  \i0\f0 so that it's more or less important.  When I did that I would get a different answer.  So now I have 3 Bhutan and only two Assam.  So it would change the value.  \par
\par
How would I know the best scaling here?  \par
\par
[Professor Laughing.] \par
\par
Yes!  You have to build it and test it.  You must use N-fold cross validation to know for sure.  \par
\par
So we've learned about k-nearest neighbor and essentially we've learned about something called an ensemble classifier.  Given an unknown data point, I find K other data points closest to it.  Then each one of those data points votes.  The classes of those data points vote for the most likely class.  \par
\par
And each one gets one vote.  If I have 5, I have 5 votes for what's the best class.  It could be 5 to 0, 4 to 1, 3 to 2 . . . but one of those will happen.  The most likely vote wins.  \par
\par
Now I can hear some of my inquisitive students out there who are thoughtful thinking that's not fair.  What if some data points are closer to others?  Maybe they should get more weight.  That's been done.  It's called fuzzy k-nearest neighbors.  So closer data points get more weight.  \par
\par
The closer data points get more influence and more importance.  Each one votes once.  But some of the votes count more than other votes.  \par
\par
So they have -- you can think of them as votes but with different weights on them.  I have to keep moving along.  \par
\par
And there's different ways of doing the fuzzy distancing.  You can use a bunch of things.  \par
\par
Here's an application.  I want to design an airfoil.  There's like 6 or 7 different parameters for an airfoil.  Lots of choices.  And I want to estimate the lift it will have.  I could build all these.  They're all slightly different.  I could build each of these or I don't have to build all of them.  I could build some of them and guess the rest.  That's going to save a lot of money because it takes a long time to build these things.  \par
\par
So k-nearest neighbors can be used for prediction.  When I was working at Kodak, I didn't but one of my neighbors used k-nearest neighbors to predict ahead of time what a chemical compound was going to do.  They wanted to design a new film.  When you run an experiment and build a film, there's about 27 layers in the film.  \par
\par
It's very expensive jello.  That's what it is.  Instead of running an experiment that costs $100,000, they wanted to predict ahead of time what is the best organic chemical?  Using data from a known database they tried to find the best chemical compound.  And then they had to build it and test it.  They were aiming for a specific color behavior for that film.  They had certain aims and test points.  \par
\par
So they actually did this.  And it worked.  \par
\par
So we've talked about this problem of getting the number of nearest neighbors.  You have to compute the distance and so there's faster techniques out there.  You'll see libraries talked about.  These libraries will be called something like FLAN.  Fast local approximate nearest neighbor.  That's out there.  \par
\par
Oh so here's this problem.  ANN could mean approximate nearest neighbor or artificial neural net.  So watch out for that.  \par
\par
So, how do we make this faster?  We use approximate nearest neighbors.  Essentially you build a kd-Tree.  And the short of this is you build a decision tree that says given a different region, here's the data points we think are nearest to you.  \par
\par
Then, we backtrack one step and prune off the bottom.  And use that for searching for the nearest neighbors.  \par
\par
Instead of going down the decision tree and \i the \i0 nearest neighbor to use, it goes down to the decision and gets a \i few\i0  nearest neighbors to choose.  Those are the approximate nearest neighbors.  That's the high level understanding of it.  It gets more complicated than that.  I won't get too into it.  \par
\par
But you should know there's things called kd-Trees and ways of speeding up nearest neighbors.  So we've talked about k-nearest neighbors.  There's two things -- oh I forgot to write this down -- there are two things that slow down KNNs.  You should write this down.  \par
\par
The first is if you have a huge amount of data.  Even if you're using a fast k-nearest neighbor, it still takes time to sort through all the data.  So if you have a lot of data -- you have a huge number of comparisons to do.  \par
\par
And, the other thing will be if you you've got data that has 100s of attributes, then I have to compute differences for all the attributes.  Either large amounts of data will slow it down, or a large number of attributes will slow it down.  \par
\par
OK?  \par
\par
Alrighty.  I think that's it for me for right now.  Do you have any questions?  \par
\par
[Professor Laughing.] \par
\par
Student:   I have a question about the homework.  \par
\par
Professor:  Hang on!  Let's finish the lecture.  \par
\par
Student:   If you have too many attributes, how do you trim them down?  \par
\par
Professor:  That's a good question.  That's called feature selection.  It should be called attribute selection.  But it's called attribute selection.  The people who did feature selection wanted to work generically.  If you take an attribute and multiply it by 1 you have a feature that is the same as the attribute.  So attribute selection is the topic there.  \par
\par
We will talk about that later on.  \par
\par
[Recording stopped.]  \par
\par
Questions about the homework?  What is the homework?  I forgot.  You can remind me.  \par
\par
Student:   There's a part where you ask us to use the cross correlation coefficient from a library.  And Pandas uses that.  But it's labelled as the Pearson, Spearman, or Kendall's cross coefficient.  Are any of those OK to use?  \par
\par
Professor:  You want the Pearson's cross correlation coefficient.  And as we get into now -- right so PANDAs has access to designs by statisticians and they know about different cross correlation coefficients than I know about.  So I'm outside of my field of expertise.  I would say use the Pearson's.  And I don't even know what the others are.  I'll admit it.  That's the statisticians world.  \par
\par
If I want to know about that stuff I ask Ernest.  \par
\par
Now, if there's an attribute in there somewhere, which has no cross correlation coefficient, it has a cross correlation coefficient of 0, could it be used to predict the target variable?  Is it likely it would be used for the target variable?  \par
\par
No.  Doesn't matter what your target variable is.  If you've got some attribute, how often do people buy tuna fish?  It can't help you predict anything else.  It cannot help you predict peanut butter, steak, chocolate chips, whatever.  It's uncorrelated.  So if the absolute value of the cross correlation coefficient is 0, you should probably throw that out.  \par
\par
I have to admit I made up this data last year and I haven't looked at it since.  I'm trying to be efficient this year.  It sounds better than lazy.  \par
\par
Has anybody figured out how many clusters they think there are?  There's two parts to the homework.  One is agglomeration using a different technique and the other is to do k-means on it.  Did anybody run k-means through it yet?  OK.  Not yet.  \par
\par
I will tell you I think there's 5 or 6 clusters in there.  I have forgotten because it was last year.  But expect 5 or 6 clusters.  When you print out the size of the smallest cluster that gets merged in, you'll see it go onesey twosy a bunch of times, then 28 or something much bigger than 2.  28, 50, 75, 30 . . . and the last numbers of those -- so if we have 4 big numbers at the end, those 4 big numbers are being merged into a 5th cluster somewhere.  \par
\par
If there's 4 big numbers at the end that's 5 clusters total.  Does that make sense?  Everybody with me?  \par
\par
OK.  Well go do the homework and you'll see it fall through.  This is one of the reasons I have you try this a couple of ways and work in pairs.  You can cross check each other.  \par
\par
OK?  \par
\par
That's it for today.  Over and out.  \par
\par
Student:   I had one more question.  Speaking of pairs . . . are we allowed to work with the same pair for the project?  Or does it have to be a different pair?  \par
\par
Professor:  I encourage you to use more people and meet more people.  But you can use the same pairs.  For the project you can use 3 or 2 people.  OK?  \par
\par
Student:   Thank you.  \par
\par
Student:   One more question.  On the myCourses page, you have a project for 2201.  Which is I believe last semester.  We should just read through it and understand it?  Is it going to be the same?  \par
\par
Professor:  It'll be different.  I have not updated it.  I haven't figured out how to assign groups yet.  I won't figure that out until tomorrow.  \par
\par
Student:   OK.  Thank you.  \par
\par
Professor:  Think about the traffic crash data.  \par
\par
Student:   OK.  \par
\par
Professor:  OK.  I want to make one other point before I sign off.  And that is this: if anybody needs to make special arrangements to have office hours -- I don't care if you're hearing impaired or have a job or whatever -- if you want to make special hours I can do that.  Even on weekends if we need to.  There are provisions for this.  Let me know.  I try to be accessible.  I also try to be sane, but I also want to make sure everyone is being served and their needs are addressed.  \par
\par
I can also type in a chat bar with somebody.  So we don't even need to necessarily have interpreters available.  I have a long, good standing record of helping people with hearing issues.  OK?  \par
\par
Over and out!  \par
}
 