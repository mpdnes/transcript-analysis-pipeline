{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1033{\fonttbl{\f0\fnil\fcharset0 Georgia;}{\f1\fnil\fcharset0 Times New Roman;}{\f2\fnil Times New Roman;}{\f3\fnil\fcharset161 Times New Roman;}}
{\*\generator Riched20 10.0.17763}\viewkind4\uc1 
\pard\tx620\tx1240\tx1860\tx2480\tx3100\tx3720\tx4340\tx4960\tx5580\tx6200\tx6820\tx7440\tx8060\tx8680\tx9300\tx9920\tx10540\tx11160\tx11780\tx12400\tx13020\f0\fs32 Principles of Datamining\par
Professor Kinsman\par
CSCI-420-02\par
February 16, 2021 \par

\pard\tx440\tx880\tx1320\tx1760\tx2200\tx2640\tx3080\tx3520\tx3960\tx4400\tx4840\tx5280\tx5720\tx6160\tx6600\tx7040\tx7480\tx7920\tx8360\tx8800\tx9240\par

\pard\tx620\tx1240\tx1860\tx2480\tx3100\tx3720\tx4340\tx4960\tx5580\tx6200\tx6820\tx7440\tx8060\tx8680\tx9300\tx9920\tx10540\tx11160\tx11780\tx12400\tx13020 Professor:  [Professor Laughing.] \par
\par
I'm giggling!  Amberlee has very round glasses!  [Professor Laughing.] I had not noticed that before.  Nobody would wear round glasses!  That would be silly!  Ha!  OK.  Good afternoon.  \par
\par
If you haven't signed in, please send me a note in the chat so I know you're here.  Today, let's see.  Last time we went through a long collection of distance metrics.  So it's long and dull and boring and everybody fell asleep.  Sorry about that.  It's going to be that way again today.  We're going to cover the other half of the distance metrics you should be thinking about.  We'll even talk about the Mahalanobis distance.  But first, let's think about why you should care.  \par
\par
So I'll share out desktop number 2 . . . and let's talk about some stuff we might need for the -- \par
\par
[Recording in progress.]  \par
\par
So here's some things we'll need for use on the next homework assignment.  As we go on and on in this I start giving -- the homeworks become less and less hand-holdy.  So we're going to be dealing with some of the obtuse data.  Obtuse data.  That means . . . you had a -- we were given data.  You were given this question.  And so I took the data from the obtuse quiz and I went and put it into a spreadsheet and I deleted some of the questions I didn't think were very interesting and I changed everybody's names.  \par
\par
Grace Taylor.  I don't understand who that is.  Mary Gold, James Bond, Phil O'Dendron -- that's a joke.  Philodendron is a plant.  Anna Conda.  It's just fine.  Kay Nine.  OK.  Anyway, it's not obvious how you pronounce that.  Let's make it more obvious.  OK.  \par
\par
So these were all different sort of surely joking tomfoolery.  OK.  So I looked across and I put in a list of what people said.  Let's look at some of the answers they said.  You're going to need to know about this.  So take notes.  \par
\par
How often do you exercise?  How often do you wash your hair?  Dress order.  Oh, this is interesting.  Do you put your socks on before your pants or pants before socks?  This question never occurred to me until somebody pointed out that it's faster if you put your socks on before your pants.  It's called planning ahead.  A good thing to do.  \par
\par
Polydactyl means do you have 6 fingers on your hand?  When I'm asking if you're polydactyl I'm seeing if you're an outlier.  Somebody said they have 6 fingers on their right hand.  That's weird.  It's an outlier.  Famous computer scientist?  Grace Hopper.  Favorite spice?  Blah, blah blah . . . \par
\par
When you had to pick one of 12 spices, alright?  OK.  Cool.  This was -- what's the next thing?  Hair color.  I said of these 12 colors, which one best matches your color?  Eye color.  Of these 12 which best matches?  Of these 12 colors, what's your shirt color?  And then we asked, how many namespace colors are there?  You can actually learn while you're taking the quizzes.  You can get new information.  \par
\par
OK so the correct answer is 12.  Red, green, blue, yellow, magenta, cyan, white, grey, black, and then 3 more psychological colors.  They're easy.  The RIT colors and pink.  Red, orange, and pink.  Those are the psychological colors because they only exist in your brain.  Pink is a light color, who cares?  But your brain thinks it's special.  \par
\par
I asked you your favorite cookie.  You had to pick.  Star Wars.  You should know, "May the Force be with you."  Somebody was being difficult and didn't answer their favorite cookie.  Maybe they didn't know.  \par
\par
There's a famous bear named . . . Winnie the . . . and I intentionally spelled Winnie the Pooh wrong and some people may have noticed that but it was spelled wrong.  So you could make the connection between what I said and what I meant and everybody got it right.  Winnie the Pooh.  Pooh, pooh, pooh, pooh . . . OK.  I don't want to make light of this.  I don't want to poo-poo the answers.  But everybody said Pooh.  Here's a problem.  \par
\par
I'm going to pick on somebody and since -- at a random glance -- at the top I see Sam is showing up.  We're going to get Sam -- we're going to find out a new buddy for Sam.  We're going to find out who else is closest to Sam.  \par
\par
Sam, are you there?  \par
\par
Student:   Which Sam?  \par
\par
Professor:  Binder.  They're at the top.  Let's pretend that Sam is still single and looking for a buddy.  We're going to invent a software product called DateMate.  There's a program out there you go and you fill in information about yourself and it help sure find a meet.  OK?  Sam!  So you said some answer to this question.  We're going to look over these things and find out who is most similar to you.  Are you with me?  \par
\par
Student:   Sure.  \par
\par
Professor:  Everybody here answer this question correctly: will this help you differentiate this person from another?  \par
\par
Student:   No.  \par
\par
Professor:  Of course.  So this is obvious once we ask about this.  This is the first lesson in feature selection.  We call this the invariant feature rejection principal.  If an attribute is exactly the same, it can't help us differentiate between class A and class B.  So you throw it out.  First principal.  Somebody should write that down.  We asked about the 3rd hand on the watch, which is the second hand.  That's weird.  \par
\par
Have you played high school music?  Yes or not?  I just concatenated that.  Were you involved in a high school club?  Do you know the answer to everything in terms of computer science?  Did you know the answer to the computer was deep thought?  And they later used that concept to learn deep learning.  \par
\par
Air speed of a swallow -- got it.  Here's the matrix pill.  Got it.  You either got it or didn't.  It is a binary answer.  OK?  \par
\par
So I asked 90 different questions.  What is the dimensionality of this data?  How big is it?  Mr. Binder?  Sam?  \par
\par
Student:   90 dimensions.  \par
\par
Professor:  Perfect.  Good.  But it's actually worse than that.  This favorite flavors of ice cream -- so normally if it's 90 binary data, it's 90 dimensions.  Yes.  But is me of these there are actually more possible combinations.  So we'd say the answer is 90 dimensional.  But the number of combinations is worse than 2 to the 90th.  Because some of these you could answer two different ways.  Sam, what's your favorite flavor of ice cream?  \par
\par
Student:   I'm a big fan of chocolate chip peanut butter cup.  \par
\par
Professor:  OK.  Let's go sort the data by favorite flavor.  This is a fun thing to do.  This is actually a good thing to do.  OK.  Chocolate chip peanut butter cup.  Let's see if we have that in our list.  Oh!  Here's somebody who said chocolate chip peanut butter cup.  Maybe that's Sam?  Uh oh.  Fortunately we've anonymized this.  So here's this problem.  You could put down anything you wanted in this.  It's actually more than 12 different things.  \par
\par
You could put in Cherry Garcia -- there's a cookies and cream -- dairy-free peanut butter.  Some things are overly specific.  Somebody couldn't make up their mind and said it changes.  There's a lot of chocolate chips.  I want to convert this from multi-way decision to binary choices.  1s and 0s.  I'll take this particular attribute and create and derive from it another attribute.  \par
\par
This attribute says favorite flavor is vanilla.  No, no, no -- OK so all these no's go on -- and then we get down to the vanillas and here they are.  Pay attention.  \par
\par
My right hand is hurting me and I don't want to type this all up.  \par
\par
I've converted this to one attribute.  There's another one, favorite flavor is chocolate.  There's another common one that comes out: mint chocolate chip.  Look at them all!  It's really very common.  Somebody put mint chocolate.  I'll say they probably meant mint chocolate chip.  These people who said mint -- these people are wrong!  They must've meant mint chocolate chip so I'll arbitrarily decide they're in the minute chocolate chip group.  \par
\par
I'm converting these into binary values.  For this set of variables I create, it can only be on one place.  You can only have one of these on.  You cannot have your favorite flavor of vanilla and chocolate at the same time.  They have to all be 0s.  OK?  \par
\par
And there you go.  \par
\par
Now when I'm all done with this, I'm going to put the big lumps in here and then I'm going to also add in one that says favorite ice cream flavor is . . . other.  Let's do that now.  Insert, column . . . fave ice cream, other.  OK.  Across all these variables, however many I create, you can only have one of these on at the same it.  This is -- in electrical engineering, we call this binarizing data.  In the world of data science, they call this One Hot Coding.  You need to know what that is.  \par
\par
One Hot Coding.  You're taking an attribute and changing it so it's only true for one set of binary variables.  Here's the favorite pizza flavor.  Now, we could have Sam Binder -- we could go figure out somebody else who has the same ice cream flavor or other.  So that we can find some commonality between them.  Right?  And we talked about different methods for vector differences, like Hamming difference, Jakkard coefficient, simple matching coefficient -- those are the top 3 that come to mind.  There's dozens but those 3 are important.  \par
\par
We asked your favorite pizza topping -- got it.   You should be able to go -- I'll ask you to take this data and convert it to one hot coded data.  Worst pizza topping.  This is weird.  What you will see if you were to take this and sort it by favorite flavor, let's go sort the data.  By . . . favorite pizza topping.  And then I'm also going to sort it within that -- I'll do it by worst pizza topping as well.  \par
\par
OK so look there's a bunch of people who like bacon.  Ham.  Some people refuse to do this.  OK.  They just can't think about it.  They just like cheese.  Olives.  Lots of people like olives.  Olives are a favorite for some people.  Here's pepperoni.  Look at all those people!  Peppers or sweet peppers.  Bunch of people who like pineapples.  Let's highlight those in yellow.  \par
\par
I haven't even looked at this data really.  Let me go resort it.  I'll sort it again.  This time I'll switch these around.  \par
\par
We'll just sort it by the worst type.  Anchovies.  It says fish there.  Some people really love olives and some people really don't like olives.  Here's a bunch of other things.  Oh!  Look at this!  Pineapples!  See?  On this column here, there's a huge cluster of people who really like pineapple as their favorite.  In this column here, there's a cluster of people who really do not like pineapple.  \par
\par
One of the weird things that fallen in our society.  It all has to do with culture really.  Raise your hand -- look for your reaction space in life -- raise your on the Zoom if you like the taste of prune juice.  OK.  We've got a couple of people . . . but not a whole lot.  OK?  \par
\par
Hands down.  \par
\par
Now, raise your hand if you've ever tasted prune juice.  OK!  [Professor Laughing.] there's a bunch of people.  But it's not everyone!  Which means there's a lot of people who have pre-existing judgments or biases against prune juice even though they've never tried it.  \par
\par
Neil says it's a warrior's drink.  \par
\par
OK.  Another thing we've gotten from science fiction.  A love of prune juice.  It's actually very sweet but because it has a bad wrap, most people have not tried it.  They've made a judgment but they don't have a good common basis for comparison.  They don't really know what they're talking about.  Same with anchovies.  I don't like the idea of having dead fish on my pizza.  So I wouldn't want anchovies on my pizza but I have never tried them on my pizza.  I don't have a common basis for comparison.  I don't really know what I'm talking about.  That's an important concept in data science.  \par
\par
You need to know what you're talking about.  You need to have domain knowledge.  I don't like olives.  I have tried them.  I'll pick them off of my . . . thing.  OK!  One of the answers you could have answered was mushrooms.  The question actually says, mushrooms could be poisonous.  Sam, what's the problem with this answer?  \par
\par
Student:   The mushrooms used on pizza are generally not poisonous.  \par
\par
Professor:  What's the problem with this choice saying mushrooms could be poisonous?  So, Alice is looking over the list and doesn't know what to answer.  But she says this thing could be poisonous.  Will that influence her?  \par
\par
Student:   Sure.  \par
\par
Professor:  Yes.  What do we call it if I have a question that's influencing the person?  \par
\par
Student:   Leading question.  \par
\par
Professor:  In data science, we say it's biased.  Walter has it right.  So it's a biased question.  We want to watch our for those.  When I said anchovies I think it said, which thing is your least favorite and then it said anchovies.  It didn't say fish, it said dead fish.  Which is unappetizing.  OK.  \par
\par
We have to get Sam a good mate.  We don't know if he's looking for a male or female and all the answers are anonymized.  We don't know.  But we can de-anonymize them.  That's when you take data that's supposed to be anonymous and using the figures, the features recorded, reverse engineer them to figure out who was who.  We can figure this out -- I can identify Sam Binder in here.  How do I know who he is?  \par
\par
Student:   I'm the green highlighted line.  \par
\par
Professor:  Well, no, I don't think so.  \par
\par
Student:   Scroll down.  I'm the green highlighted one.  \par
\par
Professor:  You are?  Here?  Really?  Oh, man.  Sam!  I'm so disappointed.  OK.  Are you sure?  Anyway, I didn't mean to do that but there you go.  Sam must have been in a hurry.  The youngest nobeloreate was Mahala.  \par
\par
I don't know who said Donald Trump but I don't want to meet this person.  It's not Donald Trump.  Some people answering silly questions as they go through are technically called "wise guys."  I don't care if you're male or female.  We call them wise guys.  So you'll see those as we go through.  \par
\par
I asked how many hours of sleep on average do you get?  And then later on I asked how many hours of sleep do you typically get?  Of all the hours of sleep you had in the last 7 days, what's the middle?  That's the median.  What happens is typically -- usually -- when we ask for the average people give you any old number that they think represents that.  When you say, what's typically?  \par
\par
Wait, he already asked for average and now he's asking for typical?  What's that mean?  That's the mode.  Average, median, and mode are the 3 measures of centrality.  You should know the log base 2 of 1024.  It's not 68.  Uh oh.  Somebody in green completely skipped a page.  They completely skipped all of these things.  We have missing data.  We need to fill that in.  OK?  \par
\par
How can we fill in missing data?  \par
\par
One thing I could do is I could take this person who forgot to answer these questions and delete them from the database.  If it's just one particular attribute, I could delete the entire attribute that's there.  I don't want to lose this data though so I could take the average of all the sleep that's typical and fill in a mode or average value to make sure I don't throw the average value off.  I don't want to put in like 20 here because that'll definitely throw the average off.  I want to put in a number that makes sense.  \par
\par
We can fill in 10 here.  Log base 2 of 1024 is 10.  \par
\par
I shuffled the ups and downs a little bit to help add some randomness to things.  OK.  Got it.  Now, Sam, when you go put mugs away -- I don't have my coffee mug with me -- I go to put the coffee mug away in the cupboard.  You put it in the cupboard.  Do you put it mouth up or mouth down?  \par
\par
Student:   Mouth down but I have pegs at my apartment so I just hang them.  \par
\par
Professor:  Oh . . . down.  OK.  And so now that's important.  Because then when we want to find a partner for Sam you don't want somebody who has the thing going up and then the next partner has it going down -- phew!  Oh, that would be bad.  So you want to make sure you and your partner get along well.  OK?  \par
\par
Harry Potter.  I solemnly swear that I'm to no . . . good.  And most people go to this right.  Some people I may have already corrected it -- somebody was in a hurry and put G, O, O, D, D.  They spelled it wrong.  This is somebody running through the quiz and hurrying and not giving it enough attention.  I am up to no . . . thing.  That's not correct either.  \par
\par
Somebody else -- I don't know where it is -- but somebody didn't know how to spell "good."  They put in "gud."  So we have to fix the data.  \par
\par
Can you do the VULKAN peace sign?  Shirt color?  Doesn't matter.  If you're a juggler, and you want to do pairs juggling you want a partner that can juggle.  Could be important.  First language.  Do you know how to ride a bike?  Drive a car?  Fly an airplane?  Have you jumped out of an airplane with a parachute?  And then I asked if you had jumped out of an airplane with a parachute on purpose?  \par
\par
This is important because if your life is really boring, you want to pick a spouse who is exciting.  Who will take risks and have fun.  It brings the fun in your life.  On the other hand, if your life is out of control and chaotic, you want a partner that's stable and reliable.  [Professor reading: questions on slide.]  \par
\par
When you do a toilet paper roll, do you put it in with the toilet paper coming out over the front or that it's hidden behind you?  \par
\par
Student:   Out the front.  \par
\par
Professor:  OK.  This is important.  Because if you want it coming off the front and your spouse or partner wants it coming off the back . . . Whoa!  You'll need two rolls of toilet paper in the bathroom to keep everyone happy.  \par
\par
Usually when we ask people if they're pet people, they say yes or no.  If we ask if you're a dog person and we have a dog person we expect that's exclusive.  These are usually considered exclusive.  They can't happen at the same time.  Then we asked questions about hobbies and these are the top 20 hobbies I got off of some list somewhere.  \par
\par
This is important.  Are you a morning person or not?  That's important.  So now we're going to go find clusters of people in this data set to see who is who.  That will be a process for later on.  Meanwhile, we've got to cover the -- we have to go on and cover other things.  \par
\par
[Recording stopped.]  \par
\par
[Recording in progress.]  \par
\par
We have to cover as many distances as we can.  Let's go cover some distance metrics.  \par
\par
OK we covered the standard ones before.  Let's cover the oddball ones.  The cosine between -- if I have two vectors I can compute the angle between those vectors regardless of what those vectors are.  It's wonderful.  So the cosine and the angle between them, I think I talked about this last time.  The cosine between tells us how much do those two vectors point in the same direction?  I have one that points this way and another one that points close to it.  They'll have a high cosine similarity on the other -- if I one that's pointing one direction and the other that's orthogonal to it like 90\'b0, that's 0.  And you could have one vector going in one direction and the other going in the other direction, and the cosine similarity is going to be minus 1.  \par
\par
This helps us find the difference between my two vectors.  We can use that for cosine similarity.  \par
\par
Mutual information is a formula.  And the intuition here is -- for all \i\f1 s\i0\f0  and all \i\f1 t\i0\f0 , the information between \i\f1 s\i0\f0  and \i\f1 t\i0\f0  is for every single measurement.  The probability of \i\f1 s\i0\f0  and \i\f1 t\i0\f0  happening at the same time, times the log of the joint probability, divided by the probability of \i\f1 s\i0\f0  and the probability of \i\f1 t.\i0\f0   If \i\f1 s\i0\f0  and \i\f1 t\i0\f0  are mutually independent, what does that mean?  If they're statistically independent, what is \i\f1 p\i0\f0  of \i\f1 st\i0\f0 ?  You should know this from your class on statistics.  \par
\par
You should know that means the probability of \i\f1 s\i0 ,\i t \i0\f0 -- that's the joint probability -- is equal to the product of the two probabilities.  OK.  So given that definition, you could fill this in up here with the probability of \i\f1 s\i0\f0  and the probability of \i\f1 t\i0\f0  and that means this would be \super\f1 1\nosupersub\f2\u8260?\sub\f1 1\nosupersub .\i  \i0\f0  What's the \f1 log\i  \i0\f0 of 1?  It's 0.  \par
\par
If the two things are statistically independent, the mutual information will go to 0.  So that's a good indication the two things are not helpful.  You'd typically use this for feature selection.  How much information about my target variable does this give me?  I measure the person's hair length and I go to see what is the chances of them being Bhuttan or Assam?  Whatever it is.  \par
\par
However I define my target variable.  True or false.  OK.  \par
\par
So that's mutual information.  It's a formula.  We get this concept from the world of statistics.  The other thing that we borrow from the world of statistics is the idea of a \i\f1 z\i0\f0  score.  I would have sworn that talked about this a little bit last time.  Anyway!  Here's the \i\f1 z\i0\f0  score.  I draw a Gaussian curve.  Your standard bell curve.  Here's the average value, mu.  \par
\par
So this might be how much hair does this person have over their eyes?  And that's a standard deviation.  A measure of spread.  How spread out is the bell curve?  So this is like bang length.  \par
\par
How long is the hair over their eyes?  Let's do it again.  But we'll do this for tail length.  Here in the world of tail length we get a different spread.  We have a situation where instead of having an average -- whatever the average value is -- I'm sorry I need to draw this better.  I need to be more specific about this.  So here the standard deviation is much bigger.  \par
\par
Here's my middle value somewhere.  Some day I have to get better at drawing with a pen.  Here's another standard deviation.  It's the distance from the median value to the first inflection point of the bell curve.  Where it starts going up faster and faster instead of down.  Stops curving down and starts curving up.  That's the standard deviation.  \par
\par
OK!  I want to compare these two.  I've got a point, some data point, and the data point might be out here in terms of bang length.  And I have other data points over here and I want to compare them.  What we do is we compute the \i\f1 z\i0\f0  score.  The \i\f1 z \i0\f0 score, if you haven't had statistics, is the number of standard deviations out from the center.  So for this \i\f1 x\i0\f0  in the top value, that would be 1.  2.  Let's say it's 3.  OK?  \par
\par
That's 3.  It's pretty far out there.  It's a far out outlier.  Down here if I want to compare these two in terms of \i\f1 z\i0\f0  scores this would be 1 \i\f1 z\i0\f0 -score out and then this is the same length to -- oh, look.  This \i\f1 x\i0\f0  here is not as far out.  This is much more of an inlier than that one.  The concept of a \i\f1 z\i0 -\f0 score can help us.  It's a statistical distance out from the center of a one-dimensional attribute.  \par
\par
Now, that's one statistical measure.  The other big one is the Mahalanobis distance.  This is a 2D -- it's actually multidimensional Gaussian distribution.  It's used to tell how far out a data point is but in higher dimensions.  We're going to talk a lot about this as we go along.  Here's a quick overview of it.  \par
\par
It's a multidimensional Gaussian distribution.  Let's envision that if I can.  Here is a cloud.  A multidimensional Gaussian cloud.  Within this Gaussian cloud -- oh boy.  How do I draw this?  There's a middle.  Looks like it's about there.  So I've got some one Mahalanobis distance from the center.  If I draw two Mahalanobis distances from the center, it's like this.  3 might be like this.  \par
\par
These are kind of like a two dimensional \i\f1 z\i0 -\f0 score.  How far out from the center is it?  Now you could also compute the distance between two points in terms of a multidimensional standard deviation.  Oh, yeah.  OK.  In order to do that, remember we talked about covariance last time?  Here's the formula for the covariance inside a covariance matrix.  It's attribute minus its average value times the other attribute minus its average value.  \par
\par
This is a situation where when one is up, and the other is up, you get a positive covariance.  If they go up and down in the same way, the covariance is positive.  If one goes up and the other down, it's a negative covariance.  This is telling me when \i\f1 x\i0\f0  goes up, how often does \i\f1 y\i0\f0  tend to go up?  \par
\par
They're probably positively correlated here.  If I wanted to change it from a covariance to a cross correlation coefficient, we'd divide by the two different standard deviations.  That's the big difference.  The cross correlation coefficient is always between minus 1 and 1.  The covariance could be anything.  We don't know what that number is.  It could be really big.  OK?  \par
\par
So now, this covariance here -- oh boy.  That's confusing.  That plugs in -- this entire matrix plugs in here and it's not the matrix.  It's the inverse matrix.  What does that do?  Oh, well, if these things here were nice, circular rings, and I could draw nice circular rings -- I'll start over again . . . \par
\par
If these were nice circular rings, and I'll just wing it here, and I want to compute the distance in nice circular rings, the distance out from the center would be called the Euclidean distance.  And if that was the case, when we computed the Euclidean distance, we would not have this covariance matrix here at all.  I'll get rid of that.  It'd be the distance between the two vectors.  This \i\f1 r\i0\f0  minus \i\f1 q\i0\f0  is element by element difference between the two vectors.  That's like \f3\lang1032\'c4\i\f1\lang1033 x\i0 ,\i  \i0\f3\lang1032\'c4\i\f1\lang1033 y\i0 ,\i  \i0\f3\lang1032\'c4\i\f1\lang1033 z.\i0\f0   And then you multiply by the transpose and on the right we'd have \f3\lang1032\'c4\i\f1\lang1033 x\i0 ,\i  \i0\f3\lang1032\'c4\i\f1\lang1033 y\i0 ,\i  \i0\f3\lang1032\'c4\i\f1\lang1033 z.\i0\f0   Got it?  \par
\par
If you did this, you'd have \f3\lang1032\'c4\i\f1\lang1033 x\super\i0 2\nosupersub\i  \i0 +\i  \i0\f3\lang1032\'c4\i\f1\lang1033 y\super\i0 2\nosupersub\i  \i0 +\i  \i0\f3\lang1032\'c4\i\f1\lang1033 z\super\i0 2\nosupersub .\i  \i0\f0   That's the dot product of this vector and its transpose.  You take the square root and you get the Euclidean distance.  This is exactly like a Euclidean distance except it's messed up.  It's not that way.  It's not circular and instead we have ellipses going on.  \par
\par
So the Mahalanobis distance includes this inverse covariance matrix that undoes the spread.  Essentially, it pushes in on the parts sticking out and it stretches out the parts that are in.  It converts the ellipse into concentric circles.  So you can compare the Mahalanobis distance to the Euclidean distance and you can think about it as the distance to the center.  If the \i\f1 q\i0\f0  is the origin, it's the distance to the center.  \par
\par
OK.  One really important point.  Books all over the internet get this wrong.  See that \f1\'bd\i  \i0\f0 power?  That's taking the square root.  And if you don't use the square root in there, you can't compare it to the Euclidean distance.  You don't have a common basis for comparison.  I once was borrowing Andrew Moore's slides from Google when he was the head technology officer.  I said in his slides he forgot a \f1\'bd\i  \i0\f0 and he had those slides out for many years and nobody corrected him.  He asked if I wanted a job.  \par
\par
My life would be completely different if I said yes.  But I really love teaching.  And I want my students to know you should always remember to take that \f1\'bd\i  \i0\f0 power to get a real distance and compare the distances as you go through.  \par
\par
Let's press on.  That's the Mahalanobis distance.  We usually use it to figure out how far out do outliers go?  Got it.  Sometimes when we want to do outlier removal, I want to get a figure of how spread out a distribution is.  Things far outside the Mahalanobis distance or have a high \i\f1 z\i0 -\f0 score are outliers.  I recompute the new center and the covariance and the new spread of my data and then I put the other data points back in.  \par
\par
It's a way of giving me a better measure of what the average and the spread of my data is without the outliers pulling me off.  So we do outlier removal.  Or, we just set them aside until we recompute.  Then we have to put them back in because we know they're really there.  \par
\par
OK!  Probability densities!  Sure.  \par
\par
Probability densities.  We've actually been kind of talking about this as we go.  Here's our location in data space.  I've got this distribution.  You can see that the data points are more likely to happen in the center of this mess -- looks like it's about 5,5.  I generated this data.  It's synthetic.  \par
\par
You can think of the these contour intervals -- these intervals, as if you were climbing a mountain from outside to inside, it will tell us how far is it to get from the outside to the inside?  It's a measure of distance.  If I have this data point here and I want to get from here to the center, or the top of the mountain, it's less inconvenient.  It's less inconvenient.  The closer to the top, the more convenient it is to get the center.  \par
\par
I'm thinking of distance as a measure of inconvenience.  So the contour intervals are helpful.  Everything on this line is equally inconvenient.  They're all the same distance from the center.  Don't fall asleep!  This gets exciting.  So that's the equal.  Everything is equal thing.  I've got 5 points here and 5 points here -- those points don't move.  But I'll change that distribution.  Here's a 2D Gaussian distribution in which it is spread out more along the \i\f1 x \i0 =\i  y \i0\f0 axis.  \par
\par
Everything on one of these lines is equally likely to happen.  When I look at .4 and compare it to another point -- 4 and D used to be equally likely.  See that?  They're the same distance from the center.  With this probability distribution, we go compute the Mahalanobis distance from the center -- look, 4 is much further away than D.  \par
\par
I'm going to use 4 and D because they're both inside and it's really obvious what's going on.  And so you could also make it even worse.  Now, 4 is more than 2 Mahalanobis distances from the center and D is less than in one.  Think of these as how hard is it to get from the outside to the inside?  How inconvenient is it to get from D to the center?  \par
\par
How inconvenient to get from 4 to the center?  4 has to go across many more contour lines.  And the result of that is, it's harder.  It could go the other way.  If the covariance was in the other direction, now when you look at it, 4 is much closer to the center than D.  Oh.  Yeah.  OK?  \par
\par
So this is thinking about these things in terms of distances and statistical distances.  The Bhahata coefficient.  There's differences between everything.  There's an encyclopedia of differences.  \par
\par
I'm not even going to look at this formula.  There's a formula for computing the Bhattacharya coefficient.  We use this with histograms.  I have two histograms.  What is the measure -- it's a measure of how similar are these two histograms?  If it goes up to 1, they're very similar.  \par
\par
And it's impossible to separate the data using that particular attribute.  What I would do is build -- oh, I have some histograms for my different data I want to play with.  Suppose the data on the left is the histogram based on age of maturity.  And the histogram on the right is something like, mature height.  OK?  \par
\par
So now I have these two histograms.  I want to know -- oh no.  That's not right.  I did this wrong.  Bear with me.  I have to back up.  \par
\par
Annotate, machine!  I have Assam and Bhuttan.  On the left, I have the ages of the Bhutan.  Let's make them alphabetical.  Ages of Assam.  On the right I have the ages of Bhutan.  OK?  \par
\par
Now I can use the Bhattacharya coefficient to find the difference between these.  I can figure out how much does the age help me separate these two?  That gives me a good clue about how good the ages are.  And how age will help me distinguish between the Assam and Bhutan.  \par
\par
Let's do this again.  \par
\par
This time, this is going to be the height of Assam.  And down here we might have height -- OK.  Everybody mute yourself.  Jordan.  Mute yourself.  Here's the heights of the Bhutan.  Before we were looking at two different histograms for age.  \par
\par
Here I'm looking at two different histograms for height.  The more different those histograms are, the better that particular attribute helps me distinguish between Assam and Bhutan.  \par
\par
Yeah I spelled it wrong, probably.  It's not important.  \par
\par
The important thing is you can span the semantic gap between class A and class B.  OK?  \par
\par
Good.  \par
\par
So, got it.  And then this last example histogram here is uniform.  That's spread equally between all possible bins.  So now I can compute the Bhattacharya between one and 2.  And when I do that for this data, I get 0.99.  They're very similar.  I would have to look at the formula and punch it in.  You compute the multiplication of this probability and the probability of the Assam being in this bin times the probability of the Bhutan being in this bin and you multiply one times the other and take the square root and then add it to the probability of Assam being in this bin times the probability of Bhutan being in this bin.  \par
\par
Multiply those two, take the square root, add them, and then you get 0.99.  They're very similar.  By comparison, the difference between 1 and 3 is 0.17.  This is a measure of similarity.  When it goes to 1, the two things are identical.  When it goes to 0, they're completely different.  In fact -- oh!  So, again, in terms of feature selection, I could write a program that computes histograms for each attribute.  \par
\par
And then doing that I would then run it and have it tell me which attribute has the lowest Bhattacharya coefficient between the Assam class and the Bhutan class.  I could then say the attribute that has the lowest Bhattacharya coefficient between these two -- the attribute that has the most difference between class A and class B -- that's the one I want to use.  OK?  \par
\par
And so here's a pile of coefficients.  \par
\par
Now, there are many differences.  In terms of social networks, I want to see how similar is one Sam from the other Sam?  Or Jordan from whatever -- anyway.  You go out and look at their social networks.  Are they friends?  That's a big clue.  \par
\par
We'd look at some things that go into an adjacency matrix.  How many times do they like the other person's posts?  And on and on.  This is really key.  \par
\par
So, if Lucy has a birthday and I don't wish her a happy birthday on her birthday on Facebook, they say, "oh, he must not really like Lucy!"  \par
\par
I stopped seeing Lucy's posts.  The algorithm says we must be different.  And on and on.  There are differences for homophones.  How different do two words sound?  \par
\par
Computers have a hard time with the difference between 'two' and 'too' and 'to'.  And course versus coarse.  So, is this class a course or a coarse?  \par
\par
What do you folks think?  Yeah.  I have to go check it.  Of course it's course.  If this class was coarse, then it would be really hard.  I would be giving you a hard time.  At least that is what I heard.  It's very "deer" to me.  Anyway.  \par
\par
Edit distance is used for spell checking.  I type in a word, I don't know what it is.  I'm not exactly sure.  And I look at all the possible spellings.  Here's this problem.  Sometimes I do it and it comes up with a valid spelling.  \par
\par
And I think I want to say I'm "up to no guda."  I don't know the correct spelling.  I accidentally put down the word "guda" and they say that's a word.  It's a type of cheese.  The spell checker goes on.  My biggest problem is double letters.  I have that form of dyslexia.  So my trick is I put in 3 and look at the options to see what's right.  \par
\par
I look at those 3 that pop up and I know that none of them are very good.  I would say, "oh, none of those might be right.  I better fix it!"  So I have my own trials and tribulations.  My own ways of getting around and compensating so my dyslexia.  What's the difference between this?  What's wrong here?  This is the 'oops' distance.  This is the difference -- Jasmine, what's the difference?  \par
\par
[Professor Laughing.] \par
\par
Student:   Me?  \par
\par
Professor:  Yeah!  What's wrong with this one?  What's the difference between those two?  \par
\par
Student:   You can't compare because they're not even like similar.  \par
\par
Professor:  OK so the distance is 1.  What did I do differently?  In one case I typed hello world and the other other case I typed:\par
\par
[On screen.]  \par
\par
So this used to be used.  They used to use a stylus.  It's the situation where your fingers are off the home keys.  In terms of scrabbling, that was a language.  And now you don't think about it.  You just do it.  \par
\par
It's there for finding the difference.  What did you really mean?  The computer can figure this out and fix it for you.  That's one of the things that the spell checker fixes.  Hopefully it uses the right word.  \par
\par
OK.  Strange distances number 4.  The distance to money.  This is used in psychological experiments when separating people from their money.  If I don't carry a wallet anymore because I'm in COVID, who cares?  It doesn't even have cash, who cares?  \par
\par
But, if I did have cash, I would be very careful with that cash.  I have to work hard to get it.  When I go to Starbucks and I say, "What, you want me to get $6 or $7 for a coffee?"  I don't know how much it is.  I don't go there.  Because . . . I don't.  Anyway.  \par
\par
$7.  I can't just give you a $5 and get change back.  I need to give you $5 plus a couple extra dollars and then I have to tip you. . . . I won't give you cash and dig it out.  It's hard to go there.  \par
\par
Credit cards though, it's usually more significant for me.  I don't want to have a lot of credit card transactions.  I want to know what's going on.  I use my credit card all the time but I'm cautious.  I don't want to lose my credit card.  \par
\par
Now, dining dollars and Tiger Bucks -- look at that.  I've put the money in an account.  I don't have to think about.   It has already been separated.  It's not even in my bank account.  \par
\par
And then there's the iPhone.  You just wave your phone over the terminal and it magically goes away.  My bank balance magically decreases.  Like magic.  Wave of the wand and it vanishes.  The only thing worse than that is my automatic pay withdraw.  There's things I do for health insurance, parking -- I don't even know.  The money is gone before it gets to my bank account.  They take it out of my paycheck!  \par
\par
Taxes.  They come out of your pay and you don't even know about it.  You don't even feel about it.  It's separated from anything I do.  That's the distance to money.  The thing that separates you from your money.  It makes it easier to do and decreases your awareness of how hard it is for learning real money.  \par
\par
Gray codes.  That's an electrical engineering thing.  \par
\par
These are horrible things.  You have to write special code handle these situations.  I have the set differences here.  A minus B in terms of set differences is 2,4.  But B minus A is the empty set.  So the order matters.  The distance from A to B is not the same as the distance from B to A.  Set differences are a problem.  You would have to write special code to handle that and figure out what you mean.  \par
\par
Time, degrees, and cyclic events.  It depends on your application.  If 2pm to 1pm -- 2pm minus 1pm could be 1 hour.  And 1pm minus 2pm could be 23 hours.  Or, maybe they're both 1 hour.  Depends on your application.  \par
\par
Sometimes you care about the absolute difference and end up having to write a special if statement in your distance metric and you write a little procedure that handles those.  Or you use a library that handles it automatically.  Now, cyclic events.  Degrees.  You have degrees around the unit circle.  \par
\par
If you got from 359 to 1 degree, is that 358\'b0 different or 2\'b0 different?  It's the same thing.  It depends on your application.  \par
\par
You are at a longitude of 180.  What's the difference of a longitude to minus 180?  They're the same place, but it's not always the same thing.  \par
\par
If my head is turned all the way to the right -- if I was an owl and I could turn my head all the way to the right and look myself straight and I wanted to go further to the left, it could be the same thing but you can't just keep turning your head that way.  For robotics, it's important.  With robotics you have the problem called gimbal lock.  You can't keep turning it around forever.  It will break off.  \par
\par
In some cases, depending on your application, you have to do different things.  OK?  \par
\par
On the Earth the great distance is called the Haversine formula.  It's a great circle.  The biggest circle you can put around the Earth.  It's the shortest distance over the Earth's surface, or the as the crow flies distance.  The Superman distance.  It's used for computing the distance between points for GPS points.  \par
\par
And of course, for any of these things, you can form combinations.  But, usually you want to form combinations of distances.  Now you can convert a distance to a similarity if the distance is in the range of 0 to 1 -- so if it's similar I want to maximize the similarity between two things, usually, or minimize the distance -- you can convert them back and forth.  \par
\par
Here I can take a distance and convert it -- the distance goes from 0 to 1.  Like the Bhattacharya coefficient.  I convert that into a similarity by -- the Bhattacharya coefficient is the similarity.  If I wanted to make a difference, I say 1 minus the Bhattacharya coefficient.  If the distance is not in the range of 0 to 1, you divide by the maximum value.  That's how you solve that problem.  \par
\par
You make it go from 0 to 1.  We pre-normalize the data to make it work.  Got it.  \par
\par
And you can roll your own.  You weight them all and normalize by the sum of the weights.  This normalization makes it so that the -- essentially the weight for each individual distance or similarity ends up adding up to 1.  OK.  \par
\par
It's good if they do add up to 1, but not always necessary.  You just work through it.  So, if you're doing route planning, let's see if these add up to 1.  \par
\par
.7 plus .1 is .8.  These only add up to .9.  But you could have other things in there too.  If I'm doing route planning, I may penalize left turns more than right turns.  And the mileage and time or other issues.  There's other things.  \par
\par
The distance between RIT and home might be the number of left turns plus the number of stop lights.  So these weights tell me how important are each of these different factors?  If I have two people and I want to say what's the difference between Sam and the potential love interest -- we might weight some of those things really highly.  Like does the toilet paper come over the same way or not?  That's a major lifestyle difference.  \par
\par
Other things, not so much.  I have a recipe -- oh, I just made pancakes before class.  And I wanted to find a recipe for pancakes that uses the minimum number of ingredients.  So the L\sub 0\nosupersub  norm for a recipe compared to nothing tells me how many ingredients do I need to buy if I don't have the ingredients?  \par
\par
The L\sub 1\nosupersub  norm would be the total amount of all the ingredients.  If the L\sub 1\nosupersub  norm is in cups, it'll tell me how many cups of things to buy if I don't have any ingredients.  Suppose I have to measure everything and I want to use the same measuring cup for everything.  I want to own one measuring cup but it has to be big enough for any ingredients.  \par
\par
So the supremum norm tells me the maximum amount of any ingredient I need to handle.  That's for telling the difference between a recipe and nothing.  \par
\par
Suppose I have made pancakes a couple of weeks ago.  And so I've already got some recipes.  I want to know the difference between one recipe and the next.  So the L\sub 0\nosupersub  norm between two recipes is the number of ingredients that are different.  This uses cinnamon?  Oh, I made apple cinnamon pancakes.  They are very good.  \par
\par
I didn't make apple cinnamon last time so this time I had to add the apples and the cinnamon.  So the difference here as 2.  I needed to get 2 new ingredients.  But the L\sub 1\nosupersub  would be the difference in amount of ingredients.  I needed a new quarter cup.  I needed to add a quarter cup of apples and a 16th of a cup of cinnamon.  \par
\par
So quarter is \super\f1 4\nosupersub\f2\u8260?\sub\f1 16\nosupersub\i  \i0 +\i  \super\i0 1\nosupersub\f2\u8260?\sub\f1 16\nosupersub\i  \i0\f0 and that's \super\f1 5\nosupersub\f2\u8260?\sub\f1 16\nosupersub .\i  \i0\f0  So the L\sub 1\nosupersub  norm was \super\f1 5\nosupersub\f2\u8260?\sub\f1 16\nosupersub .\i  \i0\f0  We can actually do this.  And people have recipes.com places where you go find differences between recipes.  You can go there and say I have these ingredients, what can I make?  \par
\par
OK, questions one might ask.  I'll put these up, and then ignore them.  If you're watching this recorded later on, hit pause now.  You should be able to answer these.  \par
\par
Discussions for later.  Converting similarities to distances.  Strange forms to remember.  All of these happen.  And they happen and they're application dependent.  You can create your own distance metric which suits your needs depending on your application.  \par
\par
Other ideas.  Again, over all these distances we're talking about, we think of the distance as a measure of inconvenience.  What are the 3 nice properties?  What's the Haversine distance?  There's an encyclopedia of distances out there by Deza and Deza.  A husband and wife team.  You can get that through the RIT online digitally.  \par
\par
Here's another after thought: when I sort things in one dimension, it's easy.  With two dimensions, suddenly it's complicated.  We'll want to learn how to do dimensional reduction and we'll do that.  And we'll do dimensional reduction any one of a number of ways.  We'll project on to a vector using the dot product.  \par
\par
One of our favorite ways of dimensional reduction will be principal components analysis.  And we'll get there.  It's fun.  And amazing.  And we'll teach you how to do it the right way.  \par
\par
It's been a long lecture.  Go forth and do well.  Sorry to go over.  Goodbye!  \par
}
 