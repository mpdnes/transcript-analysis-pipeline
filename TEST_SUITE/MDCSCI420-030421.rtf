{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1033{\fonttbl{\f0\fnil\fcharset0 Georgia;}{\f1\fnil\fcharset0 Times New Roman;}}
{\*\generator Riched20 10.0.19041}\viewkind4\uc1 
\pard\tx440\tx880\tx1320\tx1760\tx2200\tx2640\tx3080\tx3520\tx3960\tx4400\tx4840\tx5280\tx5720\tx6160\tx6600\tx7040\tx7480\tx7920\tx8360\tx8800\tx9240\f0\fs22 Principles of Datamining\par
Professor Kinsman\par
CSCI-420-02\par
March 4, 2021 \par
\par
Student:   Professor you're muted.  \par
\par
Professor:  Ha ha!  It's working!  \par
\par
I have radio control.  See?  I can turn the lights on and off remotely.  Sometimes.  \par
\par
OK.  We've got things I have to get set up.  We have to share out some screens.  That's good.  I'm not recording yet.  Everybody ready?  \par
\par
Carol, ready?  Give me a thumbs up.  \par
\par
Interpreter:  All set.  \par
\par
Professor:  OK thanks!  Cool.  Today we are going to go through -- today we're going to go through \i\f1 k\i0\f0  means.  And some of you think you've had \i\f1 k\i0\f0  means before.  I'll tell you that it was wrong.  It was -- or, you've done it and forgotten it.  It's one of the most important algorithms ever invented.  \par
\par
So we'll go through it again and this time you'll learn something completely different.  And I'll show you the power of the clustering.  Last time we were doing classification.  This time, clustering, we don't know.  We don't have labelled data.  We don't know what's going on.  \par
\par
\i\f1 k\i0\f0  means.  Some of these slides are redundant with things you've seen in agglomeration.  So we'll go through these faster.  \par
\par
I took this picture yesterday just before section 1 met.  What do we have here?  What are you looking at?  What do you think you're seeing?  \par
\par
Snow on asphalt.  A thrown snowball.  Snow.  Everybody is saying snow.  What we have here is my best version of what I call a random variable.  It's not rice.  It's snow.  I threw a snowball and it broke up.  There on the pavement we have a random variable.  \par
\par
Random variables have been explained to me dozens of times.  Every time somebody says something to me I can't understand it.  They describe it in terms of mathematical terms that don't make sense.  \par
\par
A random variable is a distribution that describes how things will be spread out.  Since we're talking about clustering today we'll be able to find this a cluster in our data.  So there's one cluster in there.  \par
\par
There's no easy way to know exactly where things will happen you wouldn't know this would be there and this would be there -- it's a random variable.  These little dots of stuff that get spread out, that's random.  We know kind of how it's going to spread out but not for sure.  \par
\par
OK.  You're right.  It was snow.  Let's press on.  \par
\par
Now, how many clusters do we see here?  Andrew?  How many do we see?  Carson?  You see 3.  Other people say 3.  Good.  We agree.  \par
\par
We as human beings would say, "Eh, it looks like 3."  But, people who like to argue may argue this one here on the left, that one there is another cluster.  And so there's really 4.  Or you might argue there's 5.  Or that maybe there's another little oddball one out here and that's 6.  Oh, it's hard to know.  We as humans look at this and say yes, 3.  \par
\par
Now the question is, how do we tell a computer that there really are 3 there?  How do you get the computer to learn this?  There's many different ways.  But one is clustering and one of the most common ones is called k-means.  Why do we want to be able to do this?  \par
\par
One reason is we might have an image and there in the image we want to be able to do object segmentation.  I'm going to break the image up into different things in the image.  \par
\par
And if the things are all the same color or the same texture, then I can segment them by that particular attribute and find the objects.  And do object segmentation.  I might also want to do background removal.  \par
\par
Background removal is I discover some features that help me identify the background.  If I can say I've identified the background then I can remove that and what's left is the interesting stuff.  You segment the image into the boring and the interesting.  Boring gets thrown away and then there's interesting things we want to play with.  \par
\par
The other thing we might want to do is attribute selection.  I gave a homework recently in some class somewhere where there was two different clusters of data.  One cluster of data liked chocolate chip cookies and the other liked oatmeal raisins.  \par
\par
I forget how it was all set up.  One liked to drink tea and the other coffee.  If you did that, and segmented using different attributes, you'd get well-defined clusters and easy to see.  But, because there were so many other attributes, the signal got hidden in the noise.  \par
\par
So if we ran a clustering algorithm where we tried a couple of clusters and features and different features and different features and found the ones that gave us the best clustering measured based on sum of squared errors or sum of cohesion then we could figure out the importance of those features for cluster understanding.  \par
\par
Very often when k-means is taught, or mean shift or any other algorithms are taught, they use \i\f1 x\i0\f0  and \i\f1 y\i0\f0  as the inputs.  But it's not that.  We don't often have \i\f1 x\i0\f0  and \i\f1 y\i0\f0  that we care about.  In data science, we've got attributes that are going in.  \par
\par
And sometimes we absolutely don't care about the \i\f1 x\i0\f0  and \i\f1 y\i0\f0  locations.  We just care about the other attributes being used.  For example, when I worked at Kodak we segmented our population or clustered the population of customers into 5 different groups.  \par
\par
That was about as many as they wanted to get their mind around.  They didn't want to have more than 5 groups.  They only wanted 5 and they had different market analysts specializing in different groups.  Somebody would handle the group that was professional photographers.  There was another group that was fashion.  People who bought it based on the color, sleekness, or look.  Or whether it fit in a purse.  And so on.  Different cluster centers.  \par
\par
\i\f1 x\i0\f0  and \i\f1 y\i0\f0  is how this is taught.  But I want you to think about the generic features used for clustering.  Instead of color, we may use texture.  Or some other attribute.  Here I took a picture of myself in front of a brick wall because I think it's hilarious camouflage doesn't work on bricks.  I think our local ROTC should be issued suits that looks like red brick.  But that's me.  \par
\par
We can do the segmentation here using some measure of texture.  Something along those lines.  \par
\par
So here we go.  \par
\par
We're going to talk about clustering.  At any point years from now anybody should be able to bump into you and say, "So why do we cluster Again?"  It's to understand something about the structure of our data.  \par
\par
Once we have that, we can gain knowledge about the data.  We can go forth and get the answers.  \par
\par
And different techniques lead to different kinds of understanding.  Oh!  So, when nobody is looking, I like to play with electrical engineering.  I like to play around with transistors.  Or resistors.  Let's look at transistors.  I sat down and I wanted to learn about transistors.  They explained in the books there's this and that type.  And I just got them all confused.  \par
\par
So I formed clusters.  Two big clusters of transistors.  And each of those clusters breaks down into other subtypes of clusters.  I built myself a hierarchy.  A special kind.  Within transistors I have the binary junction transistors, BJT, and field effect transistors.  The BJTs break down into NPN and PNP.  That doesn't matter.  Whether it has to do if we're thinking about the electrons following from the emitter to collector or the positive current flowing from the same thing.  \par
\par
I can gain insight very quickly.  And there's two types of FETs.  I've completely forgotten about them but there's JFETs and MOSFETs.  \par
\par
Here's the point.  Clustering can make things easier to understand.  I'm not going to ask you about particular types of transistors.  But I make ask about why you want to cluster a transistor.  To make it easy to understand them.  And I did this for myself.  I didn't have a professor looking over my shoulder saying I should cluster these.  \par
\par
I said I wanted to build a taxonomy, a particular type of hierarchy.  I take things and break them down into different fields and subsets.  Kind of like living things.  Phonemes and genomes . . . I don't remember them.  \par
\par
Taxonomy is a hierarchy.  Each cluster has subclusters and it helps me understand the data.  I have understanding of my data.  It's not deep learning.  \par
\par
And I did this for transistors but you can do it with anything.  You can do it with hammers, wrenches, wood planes . . . there's table saws.  Within table saws there's 5 different designs for table saw blades.  Business.  Understanding customers.  Medicine types of illness.  Information retrieval.  Oh yes.  We want to retrieve information or get a particular type of thing off the shelf.  The robot c have things arranged by taxonomies and it could help them arrange the data.  \par
\par
So we'll talk about k-means.  K-means also called k-means.  C-means, vector quantization, sometimes called Lloyd's algorithm -- here's the point.  This has been around for a while.  Somebody got their PhD in vector quantization.  They ran k-means on it.  It's just k-means but called it vector quantization.  \par
\par
When you search for it, it could be called something else.  And different things depending on the use or the intended application.  \par
\par
C-means is more likely to be used by electrical engineers.  But also used by biologists.  \par
\par
OK!  Here's an interesting image.  I took this picture because I almost always carry around a camera.  A real one.  Not a cellphone.  It takes me forever to get the images off my cellphone.  This stuff here you just open up the back here and this thing slides out and the next thing you know, there it is.  Those are the images.  \par
\par
So, the data transfer rate is huge.  You just like -- woosh!  Take the card and move it from place to place.  This was written on the walls in the men's room of the 3rd floor of building 76.  They wrote down -- you can read it.  They wrote these walls are so easy to clean.  For the fun of it, I took a picture and assigned it as homework.  I knew the answer and what it said and the clustering.  I'll show you how that goes.  \par
\par
Data science thing.  We caught the person who did this.  We did!  They were put on academic reprimand for something.  How did we catch this person?  You're computer scientists.  \par
\par
Handwriting analysis.  Interesting.  That would mean we had to collect a handwriting simple of everybody in the building.  It was easier than that.  No, easier than that.  \par
\par
There was no cameras in the bathroom.  Are you crazy!  Ha!  No.  As much as the security people would love to have cameras in the bathroom there are no cameras in the bathroom.  This happened like late at night on a Saturday or Sunday.  They were studying for an exam.  Cameras outside the bathroom?  Oh you people are so clever.  \par
\par
No.  \par
\par
OK.  We used a camera.  But not a camera with a view of it.  Here's my camera.  Here's my cellphone.  And when my cellphone -- ha!  Not my cellphone.  When most of us have a cellphone and we bring it on campus, what does it connect to?  Yes.  It connects to WiFi.  \par
\par
When it connects to WiFi it authenticates.  And so it wasn't much trouble to say, "Gee, who authenticated their cellphone on a Sunday night on the 3rd floor?"  Before you knew it, we had it.  Other people do this too.  \par
\par
When you go into Walmart -- if you connect to a public WiFi system, Walmart tracks you.  They know who you are, where you are, and where you're going.  I took this picture and fed it into a clustering algorithm.  First, let's look at the attributes.  I'm not using \i\f1 x\i0\f0  and \i\f1 y\i0\f0  here.  I just use the amount for every single pixel.  I have the amount of red, green, and blue.  Feeding this into my visualizer, I noticed there's a lump that's big like this.  \par
\par
I can't tilt it but it's tilted.  And that tilted thing goes from absolutely no red, green, or blue on up to lots of red, green or blue.  Along this diagonal axis, this axis is the axis of -- oh, what 3 namespace colors lie on this axis?  Anybody figure that out?  Right.  \par
\par
Sam has it.  White, grey and black.  This axis is the amount of greyness.  The other lumps, this is the green writing.  You just have to say everything along this axis here, that's the silver background of the back of a wall so that's the background.  We remove that and what's left is the green writing.  We did that automatically.  But we can do that with other things.  \par
\par
It doesn't need to be done with just -- I don't need to visualize it.  I can automatically figure out the data.  \par
\par
Here is a secret message.  I wrote this out.  This is all written in my own handwriting.  There's a secret message hidden behind this.  If you have seen these on cereal boxes, you write across it in blue and red and then you need special glasses to read the secret writing.  We can do that even better.  \par
\par
In the world of computer vision, one of the first things people think about is that even in image science -- in imaging science they want to read the red ink and look at the red plane and separation.  That will probably reveal the red ink.  \par
\par
But no.  It doesn't work.  \par
\par
Kevin, you still there?  \par
\par
Student:   Yep.  \par
\par
Professor:  OK so what if I look at this in the red color channel?  Is the red going to pop off?  \par
\par
Student:   Yes.  \par
\par
Professor:  Ah!  OK.  We need to send you back to 431.  OK in the red color channel here, we don't see the red writing.  And that's because red ink is all red and so it's all on.  The white stuff is all red all green and all blue.  So when I look at the red channel, there's no contrast in the red.  \par
\par
When I go to find something, I want to figure out what has the highest contrast there.  It's OK Kevin.  You'll remember.  It's wonderful to make mistakes.  When you take a course, any course, even this one, you come out and remember about 5 to 10% of the information.  When you take it again, you learn more of it.  This is why I let some students take 420 and then they take 720 from me and relearn the material even better.  \par
\par
If I look at the red channel I can see the world blue and maybe the word green.  In the green channel you can't see the green -- oh, that's the green channel.  OK.  It doesn't work the way you would want it to.  It's not as simple as you'd think.  But, k-means can separate these colors out.  \par
\par
I said, go find me a couple of clusters.  This was the first cluster that came out.  It's the white.  Everything that is white in the image here is the white -- it's closer to the white background than anything else.  \par
\par
The second cluster says anything that came out here is closer to the color red than to any other color.  There's a blue cluster.  Anything here is closer to the blue cluster than anything else.  Here's purple.  Nobody can read through purple writing because purple is tough.  It's a mixture of red and blue colors.  That's why there's a purple conundrum.  Do I have my purple cloth around?  I think I lost it.  \par
\par
Here's the hidden message.  And I just fed this into k-means.  There's another secret stuff.  Around all the writing on most of the inks, there's this white stuff that's kind of hidden and kind of grey-ish.  That's another color -- kind of grey-ish yucky stuff.  I call that noise.  I created a class that actually captured the noise in my data.  \par
\par
I don't know what that is, so I set it aside and ignore it.  To be ignored later.  OK?  \par
\par
K-means can solve lots of problems.  Not just for computer vision, but also computer science.  These are different attributes.  What are we looking at?  The age when somebody's hair turns gray.  Here's the adult height of something.  Oh!  It's snow folks.  The abominable snow folks.  We can look at these people and say, if you are above a certain line to the left here, you're probably in one cluster.  If you're below that, probably in the other cluster.  It's an approximation.  It's wrong.  But that gives you a good guess as to what is happening.  \par
\par
On the axis, you don't see \i\f1 x\i0\f0  and \i\f1 y.\i0\f0   It's taught as if this is \i\f1 x\i0\f0  and \i\f1 y\i0\f0  but it's one attribute and another attribute.  In reality, it's not one attribute and another, it's several different attributes that you would use.  You'd use multiple dimensions.  \par
\par
Before I start feeding my attributes in, I might want to pre-normalize them.  You should have all had something for data cleaning and normalization.  There are 3 types of data normalization I use all the time to make my data go between 0 and 1 or something else.  Whatever it is.  \par
\par
Let's talk about dynamic ranging first.  I get in a value.  An old value.  From my old whatever that value used to be.  I look at the old values they were before and I find the minimum value.  I subtract off the minimum value from the old values and then I divide it.  I normalize it.  I normalize it by the maximum minus the minimum.  \par
\par
This makes my output -- new values -- transformed normalized values of this -- what's the minimum I could have?  The minimum value I could have is if \f1 A\sub\i old\nosupersub\i0\f0  was the minimum and the output would be?  0.  \par
\par
On the other hand, if the old value was the maximum, I would have [Equation on Screen.]  \par
\par
I would get one out.  This forces the output values to be in the range of 0 to 1.  If I do this with different attributes, they're commensurate.  They're in the same range.  I don't need to do that for pixel data because it's in the range of 0 to 255 or 0 to 1 and comparable to each other.  I may want to transform it into another color space.  Like LAB.  \par
\par
Or, YCBCR.  Or something that separates out the luminance portion, the greyscale from the colorness.  So that's dynamic ranging.  \par
\par
The second technique I sometimes use is zero centering.  It's very similar.  You see the same formula showing up.  The only difference is I multiply the whole thing by 2 and subtract 1.  Instead of the range 0 to 1, what do I end up with?  What's the new range?  \par
\par
Minus 1 to 1.  Exactly.  Good, Owen.  Owen and Sam are on the spot.  \par
\par
So, this gives me values in the range of minus 1 to 1 and great for artificial networks.  \par
\par
Z-score says I take my old values, subtract off the old mean value, the old average, and I divide by the old standard deviation.  This gives me values that are based on the z-score.  This is a statistical measurement.  It gives me a statistical distance from how far away is it from its old mean?  This is commonly done in statistics world.  We do it also.  \par
\par
That's a statistical distance.  \par
\par
OK!  For the z-score, you got values that could be from minus infinity to infinity.  But 68% of the data, if it's really a Gaussian distribution, 68% of it is between minus 1 and 1.  So it's a good thing to know.  \par
\par
So, talking about k-means, how do we do this?  Don't write this down.  If you do, save an inch up here.  You've got something else that goes around this kernel.  This is the generic kernel for k-means.  You select \i\f1 k\i0\f0  data points at random.  Randomly.  And with those serving as prototypes for your new clusters.  Then, we repeatedly assign all of the date at the closest cluster center.  \par
\par
Oh.  Now, I have new clusters that have formed.  I want to update the cluster centers.  So I take each of those clusters and update the cluster center.  If I'm using the center of mass, I would -- if I'm using the average value, I would add them up and divide by the number and I get some new centers.  And then I repeat the process until I stop.  \par
\par
There's a couple of different stopping criterions one might use.  I can stop when the centers stop moving.  Or I can stop when the same data points keep getting reassigned to the same data.  \par
\par
That's a good question, how many clusters does k-means return?  That's a problem.  \par
\par
I tell it to find \i\f1 k\i0\f0  clusters.  With a secret message on the wall, I said go get me 6 clusters.  I started with 5 and got a bunch of noise so I threw another one in there.  So it always returns \i\f1 k.\i0\f0   OK?  Got it?  See what I did there?  Going to remember that?  \i\f1 k.\i0\f0   \i\f1 k\i0\f0  clusters.  OK.  \par
\par
So measures of central tendency.  You know about mean, median and mode.  You should be familiar with those.  The centroid is almost like the mean but it's the multidimensional center of mass.  The centroid could include some weights.  It could include some amount of importance.  \par
\par
It could have a situation where one attribute is more important than another attribute, so it gets more weight when we compute the center of mass.  The other is the medoid.  Sometimes our clusters are weird shaped.  When we go find the center of mass of some of these clusters here, the center of mass of this one might be out here.  \par
\par
Or the center of mass to this other one might be in the middle of nowhere.  I'm exaggerating, but that's the point.  The center of mass might not be part of the distribution.  If I go to this location in dataspace or feature space, and I said get me a point here, there's no data there.  It doesn't exist.  Instead of that, I'm likely to use the medoid.  The actual data point closest to the real center of mass.  \par
\par
That gets me a real data point to use.  OK?  \par
\par
So that's the reason why we'd want to find the medoid.  And use that.  If I want to use a real data point that's a true data point.  \par
\par
OK.  This is redundant.  We've seen this before.  You've seen this problem before.  The problem that clusters are ambiguous.  We want the clusters to be tight and compact and far from other clusters.  You might argue there's two clusters here.  Somebody else may say 6 and somebody else 4, depending on how much they want to argue.  No, it doesn't depend on that.  It really depends on what is the number of clusters that solves the problem you're dealing with?  \par
\par
What is the \i\f1 k\i0\f0  that works best for your data?  \par
\par
OK.  \par
\par
There's the top down approach where we take the data and recursively divide it into smaller pieces.  Bottom up.  That's agglomeration.  Everything starts off in its own cluster and we start agglomerating.  There's this other thing, guess and check, and then we fix it.  That's where k-means falls into.  It's the random guess and check technique.  Believe it or not, it's random.  It's starting off randomly.  \par
\par
When you get a result, the result you get depends on the random starting point.  It could be wrong.  \par
\par
So the main algorithms we're dealing with in this class are agglomeration where we take things and pull them together, the last agglomeration homework didn't work well.  We'll do that again with better data.  \par
\par
K-means.  That's today's topic of conversation.  Bisecting k-means.  I'll talk about that in a bit.  That's starting at the top with everything in one cluster and using k-means over and over until you get \i\f1 k\i0\f0  points.  DBScan.  The density based scanning situation.  \par
\par
Clustering.  This is redundant.  It's a partitioning of the data.  Hierarchical versus partitional -- we've talked about this.  We've started with a seed, kept growing it and then we get a dendrogram on the right.  \par
\par
This agglomerates that together and you might get a dendrogram that looks like this.  You've seen this before.  \par
\par
Partitioning we get nice separate partitions.  They don't overlap.  Now, the attributes that we use and the way we pre-process them are important.  So if we use -- if I cluster things based on -- and the input is dollars versus pesos, I get different clusterings.  They'll change the proximity.  That's a reason I would want to pre-normalize the data.  \par
\par
Sparseness.  That could cause issues.  The attribute types could cause issues.  Noise and outliers can throw things off.  Sometimes when I update the cluster center, I first set aside the outliers, recompute the cluster center and then throw the outliers in.  That gives me a better estimate of what the true center of the cluster is.  \par
\par
OK.  And again, we need a prototype for our clusters.  That a mathematical model.  It includes a center and an extent.  The center could be the mean, median, mode, whatever.  And the extent could be some distance in Euclidean space.  Or a statistical distance in Mahalanobis space.  Or just like the first quartile.  Like plus or minus one quartile within a given attribute.  \par
\par
One of the issues is you have to pick \i\f1 k.\i0\f0   I'll show you how to do that in a minute.  I want to remind you the initial centers are chosen randomly.  Sam goes to run this.  And Sam gets one set of clusterings for \i\f1 k \i0 =\i  \i0 2.\f0   And then somebody else runs it and they get a different set of clusterings.  \par
\par
That's normal.  This happens because you're starting off with a random seeds.  The beauty of that is you don't need to know how to start it.  You pick random points and start with those.  Each data point is assigned to the closest center.  And so on.   \par
\par
You do need to define the prototype ahead of time.  You need to know what you're doing.  I need to tell you what prototype to use otherwise I can't grade you very easily.  And you need to know what distance metric to use otherwise you'll get different answers.  \par
\par
What else do we need to know?  We need to know how to stop.  You need to say ahead of time, what's my stopping criteria?  Is it going to be when I get to a few points change from place to place, or when the cluster centers don't move by much?  If I do that, I have to define what's enough.  What's the epsilon value?  What's a small enough value that below it we won't pay much attention to it?  \par
\par
You've got to choose these values.  OK!  \par
\par
K-means -- we're going to show you some -- I will show you some animations I created.  And what I want you to see is that most of the convergence happens in the first few iterations.  So, let's go watch.  \par
\par
Here's some data.  And clearly there's 3 there.  So we're going to randomly pick 3 data points.  And if we do this right and the clustering runs, you'll get these 3 clusters.  The blue, red, and magenta.  Perfect.  If we randomly pick different versions, we get these red ones below.  Everything in the bottom ends up in one cluster.  Up here -- oh.  Bummer.  Look at that.  I have two data points in the same cluster and they're breaking them up.  \par
\par
These are both stable solutions.  In one case we chose one way and in the other case, we chose it another way.  We can actually watch that happen.  How does that happen?  First thing we do is pick 3 points at random.  Random.  Right.  How do you chose those points?  Randomly.  How do we initialize it?  What's the best way randomly?  You pick 3 points.  There's way to fudge it but basically I choose points.  \par
\par
Every single point, all these grey circles closest to the purple triangle get colored purple.  Closest to blue is colored blue, same with red.  When I do that, this is the new clustering that results.  The new centers.  \par
\par
Look at this.  Even these 3 blue ones on the right, they're closer to the blue triangle than the purple triangle.  \par
\par
OK.  Now, I've got a bunch of data in the blue range.  A bunch of data that's purple.  I compute the centers of all those blue points.  This triangle represents the center of that blue cluster.  And so it's going to move.  It's going to move down here a bit.  Down to the right because these outliers are pulling it away.  \par
\par
They'll pull it off center.  The new center for these purple points is going to be somewhere in the middle.  The magenta will probably jump to the middle.  The red thing -- yeah.  It's probably going eat up more of these purple points.  But maybe not too much.  We'll see how that goes.  \par
\par
We've computed the new centers.  Given those new centers -- see the blue one?  It's down to the right.  Not the middle.  It gets pulled off center by these outliers.  The purple goes over here . . . look at that!  There's no real data at this location in space.  That's not real data.  \par
\par
It's a mathematical model for what the data might be.  That's a prototype center.  And the red one moves down a bit.  Sure.  \par
\par
Now we repeat the process.  Everything that's closer to this new blue center gets reassigned to the blue.  And we keep going.  We keep turning the crank.  Everything closest to this blue gets reassigned to blue -- OK.  We have to repeat the process here.  \par
\par
Look at this!  See these chunks on the left?  Everything on the left, that's going to be closer to the red.  In the next iteration, I would expect everything down here to be red.  Yahoo!  It works.  \par
\par
All these data points are closer to the purple ones so they become purple or magenta, depending on if you use the correct name or the colloquial name.  \par
\par
There's no outliers in this cluster anymore.  I would expect that cluster goes back to the center of the blue region.  Also I would expect the next time we update the purple center that it jumps over to the middle of the purple centers.  \par
\par
Yes it does.  There we go.  So that's about stable.  That's the stable solution.  No changes from then.  Everything gets reassigned to the purple triangle.  Everything in the red gets assigned to the red.  Everything that's blue gets assigned to the blue.  It does not change.  \par
\par
We chose some cluster centers and we went with it and that's the clustering that gets assigned.  \par
\par
I just got done revising some of this.  If you want to see the algorithm animated, here you go.  If you're watching this with -- yeah.  That's it.  3 iterations and they're done.  OK?  \par
\par
Nothing to it.  \par
\par
So there you go.  \par
\par
I've got some -- I need to keep cleaning these slides off as I go along.  I put this setup in case somebody was looking at it later on.  That's the end of the first set of slides.  I had to break these up into several slides.  If I put them all in the same slideshow, I break PowerPoint.  The animations take up so much space that PowerPoint just dies.  I don't know why.  Don't ask me why.  Ask Microsoft.  \par
\par
So this is where we were.  We were going through this.  That's redundant.  Got it.  Let's go on.  \par
\par
Super redundant.  I thought I had another example here.  I could have sworn that I had an example here where I selected 3 other points at random.  But they weren't the same 3 points.  Oh, here they go!  You can see this is actually working along.  This is the other case we got.  This is the one where I end up with all the red points on the bottom clustered together.  That's a new stable solution.  \par
\par
It's not intuitively the best one, but that's what it is.  So there you go.  That shows you.  You can back up that and watch it again at another point in time and see how that forms.  \par
\par
So we've got this problem.  The problem is we're starting off with random data points.  How do we fix that?  By trying a bunch of different random points.  Randomly.  So, the final full k-means algorithm says this: for a large number of iteration we select K initial seed points.  \par
\par
Using those seeds, we repeat the process.  Each time I have a convergence, I'll evaluate the resulting measure of cohesion.  It's the sum of the squared error.  I go through there and I remember every single time I go through.  Does this clustering have the lowest sum of the squared error?  If it does, I'll remember the prototypes I'm using.  \par
\par
I do this like 100 times.  Like 1000 times.  Like 4000 times.  You go to lunch and come back.  Sometimes you go to sleep and then wake up and there's the answer.  Or you distribute off to 1000 different servers and let them play with points, making sure they all pick different random points.  \par
\par
When I'm all done, I get back the best set of prototypes.  Those are the ones I keep around.  Those are the ones I use.  Here's another example I put together.  So here we initially pick all of the points from the middle cluster.  So I am initially forcing it to pick a bad initial condition.  Watch what happens.  \par
\par
I believe this animation repeats.  Before long, I get the clusters you'd like to get.  It does finally stabilize to what you'd expect.  The ones in the middle are red.  The ones at the bottom are orange-ish.  Watch.  \par
\par
[On screen.]  \par
\par
Ah ha!  It's stable now.  It actually works.  It really does.  Kind of nice.  And you get some weird stuff that happens.  Here's another case where I have a bunch of people on a plane and I want to use some sort of clustering to cluster them together to see who is close to who.  Maybe this is an example.  I don't know.  \par
\par
So I have a bunch of data and it's all strung out and I want to have maybe 6 or 7 different clusters.  OK.  Let's go find 7 clusters.  Each time this goes along, the cluster centers move just slightly.  Until eventually it gets stable and they do spread out.  \par
\par
I'll save you the time.  They really do get stable and it does stop.  It runs for a long time.  That animation runs for several iterations.  But that particular animation has been known to crash PowerPoint.  I don't really want to have to get off Zoom and back in again.  \par
\par
OK.  So we keep going.  Where am I?  Oh.  I have to go to . . . lecture number S.  C.  \par
\par
In all this, we have to choose \i\f1 k.\i0\f0   How do we choose that?  What's the best way to choose K?  Here's the process.  When we have a value of \i\f1 k\i0\f0 , we compute the sum of the squared errors for that given clustering.  This is a measure of cohesion.  For \i\f1 i \i0 =\i  \i0 1\f0  to \i\f1 k\i0\f0 , that's the number of clusters where all the date appoints in the clusters, all the way up to cluster \i\f1 k\i0\f0 , we compute the distance from the center of the cluster to that data point, \i\f1 x.\i0\f0   And we get the sum of the squared errors.  \par
\par
The sum of the squared Euclidean distances from the center.  OK?  Given two clusterings, we choose the one with the smallest SSE.  Now, the problem with this as somebody else guessed earlier on is that you can always get a smaller SSE if you increase \i\f1 k.\i0\f0   Whenever you increase the number of clusters, the average error is going to increase.  It has to.  A larger K will have a lower SSE than a lower K.  \par
\par
So a good clustering with a low K will have a lower SSE than a poorer clustering with a higher value of K.  That's the ticket.  So we run this many times.  When K equals 1, you don't need to run it many times.  Just once.  Put an if statement in.  \par
\par
You will get the same SSE every time.  There's nothing deterministic about it.  Sorry, nothing random about it.  Nothing probabilistic about it.  When I use two, I do run it a couple of times and I find some lowest SSE.  I go to 3, it drops off.  Same with 4 and 5.  But look!  It doesn't drop off as much!  6, 7, and 8 they don't drop off as much either.  \par
\par
So in business, we would say there's something called the point of diminishing returns.  Beyond a certain point, you would not want to keep dividing your data up.  What's our point of no return?  Our point of demonising returns?  Alex?  David?  Anybody else awake?  You say 4?  \par
\par
OK.  I agree with you.  That's the best place to do it.  I would say it drops off and then it doesn't drop off much beyond that.  Good.  \par
\par
So that point of diminishing returns on this graph, of K versus sum of squared error, we call that point the KNEE point.  Notice how it's spelled.  K-N-E-E.  That's the KNEE point.  The name for the inflection point.  You can use a change detector to find it automatically.  Some algorithms do.  You look for the second derivative and see where it goes as close to 0 as possible or something along those lines.  \par
\par
When I ask you the name of the inflection point in the K versus SSE, I expect you to say the KNEE point.  I have some difficult students and they take online classes from somewhere else and they call it the elbow point.  Or hip point.  Those are no acceptable.  Here at RIT we're the professors that say KNEE.  You should say that to.  \par
\par
That's the answer I'm looking for when I ask a fill in the blank question.  \par
\par
Now you know how to pick K.  \par
\par
So, solutions to picking the best initial points.  You run it multiple times.  Oh, yeah.  You just do it.  You can sample and use hierarchical clustering.  You may pre-use some hierarchical clustering to get some ideas as to where to start.  We could select more than K initial centroids and then select among those final centroids as the best ones.  \par
\par
We can do post processing where we throw out the worst clusters.  You've seen where I found the secret message by including a cluster that was bad.  \par
\par
Somebody is not muted.  I hear typing.  \par
\par
Thanks.  \par
\par
And we could use bisecting K-means.  I said I was going to talk about that.  Let's talk about it.  Let's say I know I want 5 clusters in my data.  I initialize by putting all my data into one big cluster and then I randomly select two seed points.  We run k-means a couple times and split it up into two clusterings.  I select a cluster from a list of clusters, the one that has the biggest variation.  The biggest spread.  The biggest number of data points.  How you choose that is up to you.  \par
\par
One of them is too big and then I rerun it on that.  So now I've got one cluster that's too big.  I'm going to break that up into two more clusters.  I start with 1, I go to 2, I go to 3, and then 4 and I go on until I get enough K that I want.  \par
\par
So we've talked about some of these.  Pre-processing.  Normalizing the data.  Eliminate outliers.  You may want to bin your data.  There's some pre-processing that happens as you're going through the process.  \par
\par
Post-processing.  I look at it and I say there's a cluster here that just has a few data points and they're spread out and not well-connected.  I'm going to call that cluster noise.  I could say some clusters have a really high sum of the squared error.  They're not doing what I want.  Split them up again.  \par
\par
Or the opposite I have a couple with relatively low SSE and I might want to merge them together because they're really close to each other.  This algorithm has been around for decades.  Like 6.  Maybe 5 or 6.  At least.  And there you go.  You can make up variations of it as you go along.  You yourself can invent a new k-means algorithm.  Or a new pre or post processing technique.  We have to talk about some of the problems with this.  \par
\par
K-means has some issues.  First of all, you have to decide on \f1 K.\f0   That's always a problem.  We have to choose a distance metric.  If you use the Euclidean distance, you get nice circular clusters.  If I use others, I get diamond shapes.  This is the Euclidean circle you know and love.  Everything on that circle is one unit from the origin.  On the right, everything on the diamond shape is one unit on the L\sub 1\nosupersub  from the origin.  \par
\par
For example, the point that's at \i\f1 x \i0 =\i  \i0 1,\i  y \i0 =\i  \i0 0\f0 , this a unit of one away.  All of these points are one unit from the center.  Or we can have tradeoffs.  You have a point that's halfway over or halfway up -- everything on this entire diamond is 1 L\sub 1\nosupersub  unit from the center.  If we use the L\sub 1\nosupersub  distance we get diamond shape clusters that result.  And there's other distance metrics you can use.  \par
\par
As L goes up, the bulge goes up from the center.  Anything else than 1 bulges in.  L\sub 0.5 \nosupersub bulges in.  \par
\par
I have no idea Harrison what the Jaccard coefficient leads to.  I'm not even sure there is a unit distance for that.  \par
\par
Hang on, I have to cover some issues.  Got it.  OK.  We've got it.  That's the bisecting k-means.  I just talked through that.  \par
\par
Bisecting k-means, you keep breaking it up.  I have the biggest cluster on the left, I have two, it's all cyan and I break them up into two more.  You keep going and now you get 3.  I have 3 on the left and 2 on the right that are red.  And you keep going, bisecting k-means until you figure out at some point you have the right stopping condition and then it's time to stop.  Here there are 10 clusters under the covers.  That's probably what you want to stop with.  \par
\par
I would guess that the sum of squared error would level off at that point.  Other problems with k-means, it doesn't work -- clusters of different sizes can get thrown off.  It's using a distance only.  It doesn't know about size.  \par
\par
Different densities won't work.  Here, these two on the right have high density but they're close to each other so they get clustered together.  And concave shapes.  If you take these two boomerangs and you put them in you get that clustering.  Which is not what you want.  \par
\par
Instead, if I wanted to get this clustering out of there, I would use something like . . . agglomeration using a single linkage to make sure everything is closest to each other.  \par
\par
Other places . . . \par
\par
Yeah.  You overcome this usually by selecting more clusters.  And then post-processing your clusters.  In this case, I'm going to choose a bunch of clusters and look at the ones on the left and see it's the same cluster.  That's the same stuff.  Here again, I would over cluster and then look at the clusters and say, that's the same stuff.  And split it into two groups after the fact.  I do post processing.  \par
\par
OK.  K-means will never find a cluster within a cluster.  If you have data that looks like this, you wouldn't use k-means.  You wouldn't use agglomeration.  What would we use?  What algorithm?  \par
\par
The Mahalanobis is a distance.  What -- DBScan.  That's right.  We'd use DBScan in this situation.  \par
\par
OK, so questions one can ask about k-means.  Why cluster at all?  What decisions do you need to make?  Distance metric, etc.  What shortcomings are there?  How do you break it?  How can it break down?  How do we avoid that and overcome some of those?  \par
\par
OK.  I'm going to clear all of this, stop sharing our screen.  That's what I have for day.  It's been a packed lecture.  I hope I didn't wear out anybody.  I hope I -- I expect the captionists are still OK.  This gives them a good workout.  Something great to show off.  If you can caption this along as you go.  \par
\par
OK.  We have another homework coming out.  That's homework number 5.  Due Sunday night.  And you've done most of this ready.  You've done some sort of threshold detector.  You just need to recursively apply that.  Now, there's some issue you have now you didn't have before.  \par
\par
When you go to write out a if/then/else situation in python, as you get further down into the recursion, you need to indent more and more.  How do we solve that?  What do we do?  \par
\par
Yeah so . . . one tab per depth.  But you have to have something as you call it -- you need to pass in the recursion depth.  Yes.  You need to pass in a depth counter.  That routine knows how far it is.  Then when it goes to write out the if or else statement or the leaf condition, says return whatever it is, then it will know how far in to indent things.  From your point of view.  That's in.  OK?  \par
\par
So yes, you want to call that.  When you recursively call that, you call it with call depth plus 1 as you recurse.  You'll have to check your stopping criteria.  If you go too deep, you have to stop.  So get started on that.  Don't wait too long.  \par
\par
It is a bit complicated.  I have written recursion routines all the time.  But I've done this two or 3 times in my life.  I've written that particular homework answer.  And I can do it in an hour.  But, for you, there's going to be something that is going to go wrong and you'll have to go debug it.  So things will happen.  Stuff will go wrong.  \par
\par
Expect things will happen and it will be tricky to go through.  OK!  That's it for today.  I will see you all next week.  Bye bye!  \par
}
 